 \chapter{Modélisation des Circonstances Factuelles}
\label{chap:similarite}

%\epigraph{Le plus important, c'est d'avoir un langage métaphorique ; car c'est le seul mérite qu'on ne puisse emprunter à un autre et qui dénote un esprit naturellement bien doué ; vu que, bien placer une métaphore, c'est avoir égard aux rapports de ressemblance.}{Aristote, Poétique}
% 

\section{Introduction}
\label{sec:similarite:introduction}
Les circonstances factuelles sont les types de faits ou contextes possibles dans lesquels une catégorie de demande peut être formulée. Les analyses descriptives ou prédictives ne prennent sens que lorsqu'elles sont appliquées à un ensemble de décisions aux circonstances similaires. Par exemple, il serait imprudent de considérer toutes les décisions pour prédire les chances d'acceptation d'une demande de dommages-intérêts fondée sur l'\og article 700 du code de procédure civile \fg{} en cas de trouble de voisinage. Les taux d'acceptation ou de rejet peuvent être différents entre des affaires de licenciement et celles portant sur les troubles anormaux du voisinage, et même plus spécifiquement entre les troubles de voisinage entre particulier et entreprises (par exemple: chantier de construction), ou simplement entre particuliers (par exemple: troubles sonores). Il serait préférable de travailler uniquement avec des décisions similaires à la situation d'intérêt. L'identification des circonstances factuelles devient donc indispensable. Malheureusement, les faits et leur catégorisation sont extrêmement diversifiés et indénombrables.Annoter manuellement des échantillons de décisions est impossible pour chaque circonstance potentielle afin de résoudre ce problème par classification supervisée. Il est donc plus intéressant d'adopter une approche non-supervisée capable modéliser les circonstances factuelles à partir d'un corpus de documents d'une même catégorie de demande. Plus précisément, l’objectif est donc de regrouper ensemble les décisions qui traitent de problèmes similaires. La tâche peut être formulée comme une tâche de regroupement non-supervisé (\textit{clustering}).  Les objectifs de ce chapitre sont d'expérimenter des algorithmes  de \textit{clustrering} et des métriques de similarité, et de démontrer que l'apprentissage d'une distance de similarité entre documents d'une même catégorie de demande permet d'obtenir un meilleur clustering.

%\section{Formulation du Problème}
%\label{sec:similarite:probleme}

\section{Regroupement non-supervisé de documents}
\label{sec:similarite:biblio}

Cette section fait une synthèse bibliographique des différents aspects rentrant dans la conception d'un modèle de \textit{clustering}. Elle aborde principalement le choix de l'algorithme, la définition d'une mésure de similarité adéquate, la représentation des documents, la détermination du nombre de \textit{clusters}, l'affectation de labels aux \textit{clusters}, et l'évaluation du regroupement généré.

\subsection{Choix de l’algorithme de clustering}

Le clustering de documents est une tâche dont l'objectif est d'identifier, sans supervision\footnote{Sans exemples annotés.}, une structure pertinente (pour le domaine expert) dans un ensemble $D = \lbrace d_1, \dots, d_N \rbrace$ de $N$ documents non annotés en construisant des groupes représentants des catégories inconnues au départ. Ces groupes, appelés clusters, peuvent être disjoints ou à chevauchements, et plates ou hiérarchiques suivant les contraintes du domaine expert. L’algorithme à utiliser dépend généralement de la forme qu’on souhaite donner à l’organisation. 
\subsubsection{Partitionnement disjoint}
Pour réaliser des partitions distinctes\footnote{Chaque document n'appartient qu'à un seul cluster.} (\textit{hard clustering}), des algorithmes tels que celui des K-moyennes \citep{forgey1965kmeans} et celui des \textit{K-medoïdes} \citep{kaufman1987kmedoids} seront préférés \citep{balabantaray2015kmeanskmedoids}. Ces deux algorithmes fonctionnent de manière similaire, et nécessitent que le nombre $K$ de clusters soient prédéfini. Ils commencent par une définition aléatoire de $K$ centres initiaux de clusters (centroïdes) et l'affectation des différents documents au cluster dont le centre est le plus proche. S'en suit une boucle dans laquelle le centroïde est recalculé (le point à distance totale minimale avec les membres du cluster) et les documents sont réaffectés chacun au cluster dont le centroïde est le plus proche. L'algorithme s'arrête si aucune amélioration n'est plus observée, ce qui se traduit soit par l'atteinte d'une valeur minimale prédéfinie de l'erreur de \textit{clustering}\footnote{Somme des distances au carré entre les points et leur centre respectif.} ou d'une mesure d'évaluation non supervisée (\ref{sec:similarite:biblio:unsupeval}). La différence entre l'algorithmes des K-moyennes et celui des \textit{K-medoïdes} tient principalement au fait que les centroïdes du premier ne sont pas nécessairement des points (documents) de l'ensemble d'origine, mais des points moyennes des représentations vectorielles des membres du cluster, contrairement à l'algorithme des \textit{K-medoïdes} qui ne considère que les documents originaux qui ont une distance minimale à tous les documents dans leur cluster. Cette différence donne l'avantage au \textit{K-medoïdes} de ne pas dépendre d'une représentation vectorielle nécessaire au calcul de la moyenne, mais elle a aussi l'inconvénient d'augmenter sa complexité en temps  car il faut calculer et stocker la distance entre toutes les paires de documents. Il existe plusieurs autres algorithmes de clustering disjoint dont le principe de fonctionnement est différent de celui des K-moyennes. Par exemple, l'algorithme DBSCAN (\textit{Density-based spatial clustering of applications with noise}) \citep{ester1996dbscan}  ne prend pas en paramètre le nombre de clusters à construire. Il est défini sur le concept de régions de densité caractérisées par la distance minimale $\epsilon$ autorisée entre deux points d'une même région, et le nombre maximal de points qui doivent être dans le voisinage de rayon $\epsilon$ d'un point pour que ce voisinage soit une région de densité (le point central est appelé "point noyau" (\textit{core point}). Le principe du DBSCAN est de construire les clusters successivement en reliant les régions (voisinages) dont les noyaux sont à distance plus ou moins inférieure à $\epsilon$. Les points qui sont seul dans leur cluster sont qualifiés d'\textit{outliers}. 
% amélioration par réduction de dimension
En outre, le clustering spectral est une autre méthode efficace de regroupement qui effectue préalablement une réduction de dimensions à l'aide du spectre de la matrice de similarité $M \in \mathbb{R}^{N \times N}$ \footnote{$M_{ij}$ est la mesure de la similarité entre les points (documents) $d_i$ et $d_j$ du corpus $D$.} des données  avant d'appliquer un algorithme traditionnel comme celui des K-moyennes. Les dimensions du nouvel espace sont définies par les vecteurs propres de la matrice Laplacienne $L$ de $M$ \citep{shi2000spectralClustering, von2007tutorialSpectralClustering} qui peut être normalisée ($L = T^{-1/2}(T-S)T{-1/2}$) ou pas ($L = T - M$), $T$ étant la matrice diagonale déduite de $M$ i.e. $T_{ii} = \sum\limits_j M_{ij}$. 

Il est aussi possible d'utiliser les arbres de décision pour améliorer les résultats des K-moyennes. En effet, les forêts aléatoires \citep{breiman2001randomforest} permettent d'estimer la similarité entre deux points. Le principe consiste à générer un ensemble de $n$ points synthétiques, et d'entraîner une forêt aléatoire à une classification binaire supervisée avec les points originaux considérés dans la classe des "originaux" et les données synthétiques dans la seconde classe des "synthétiques" \citep{afanador2016unsupervisedrandomforest}. Une forêt aléatoire étant un ensemble d'arbres de décision (classification) construit sur des parties de l'ensemble d'apprentissage duquel on a retiré une ou plusieurs variables prédictives, la similarité entre 2 points est la proportion d'arbres dans lesquels ces points se trouvent dans le même nœud feuille. Cette métrique "apprise" peut-être par la suite utilisée dans un algorithme de clustering classique comme les K-moyennes.

%\textcolor{red}{Random Forest - processus de construction: \url{https://onlinelibrary.wiley.com/doi/pdf/10.1002/cem.2790}}

L'application de ces différents algorithmes aux documents n'est généralement basé que sur les statistiques d'occurrence des termes, et par conséquent les thématiques abordées dans les documents ne sont pas bien prise en compte, surtout que l'élimination des \og mots vides \fg{} (\textit{stop words}) peut laisser les deux documents sans sinon très peu de mots en commun \cite{kusner2015wordmoverdist}. \citet{xie2013MGCTM} démontrent empiriquement que l'intégration de la  modélisation thématique (\textit{topic modeling}) au clustering de documents améliore significativement les résultats. Cette intégration des modèles thématiques dans le clustering de documents peut être réalisée de multiples façons, mais deux méthodes semblent être les plus efficaces:
\begin{enumerate}
	\item l'intégration naïve \citep{lu2011kmeansLDApLSA} qui consiste à inférer $K$ thèmes à l'aide d'un algorithme comme le PLSA (aAnalyse sémantique probabiliste latente) \citep{hofmann1999PLSA} ou le LDA (allocation de Dirichlet latente)\citep{blei2003lda}, puis de considérer pour chaque document le thème $j \in [1..K]$ qui a la probabilité $\theta_j$ la plus élevé dans ce document suivant la distribution $\theta$ de probabilité des thèmes dans ce document; le thème choisi $j$ représente le cluster du document;
	\item le modèle thématique multi-grain de clustering (\textit{multi-grain clustering topic model}) ou MGCTM proposé par \citet{xie2013MGCTM}, dont l'objectif est d'inférer de manière jointe les clusters et le modèle thématique.
\end{enumerate}

\textcolor{red}{FONCTIONNEMENT DU LDA et du MGCTM}.

\subsubsection{Regroupement avec chevauchements}

Lorsque des chevauchements sont observables entre clusters (un document peut faire partie de plusieurs groupes à la fois), chaque objet peut être affecté partiellement à chaque cluster grâce à la notion de degré d'appartenance (\textit{membership degree}) entre un point $x_i \in X$ et le cluster $j \in [1..K]$ estimé par une fonction $u_{ij}$  \citep{baraldi1999surveyfuzzyclstering}. Il est par conséquent préférable d'employer des algorithmes de partitionnement "mou" comme l'algorithme flou des c-moyennes (FCM) \citep{bezdek1984fcm, hathaway1989fuzzycmeans}, ou le fuzzy c-Medoids (FDMdd) \citep{krishnapuram2001fuzzycmedoids}, ou la version améliorée IFKM (\textit{improved fuzzy K-medoids})\citep{sabzi2011fuzzykmedoids}. \textcolor{red}{FONCTIONNEMENT DE CES DEUX ALGO}. Le principe des algorithmes de clustering flou consiste en deux étapes principales \citep{sabzi2011fuzzykmedoids}: 

Lorsque des chevauchements sont observables entre clusters (un document peut faire partie de plusieurs groupes à la fois), chaque objet peut être affecté partiellement à chaque cluster grâce à la notion de degré d'appartenance (\textit{membership degree}) entre en jeu \citep{baraldi1999surveyfuzzyclstering}. Il est par conséquent préférable d'employer des algorithmes de partitionnement "mou" comme l'algorithme flou des c-moyennes (\textit{fuzzy c-means}) \citep{bezdek1984fcm, hathaway1989fuzzycmeans}, ou le fuzzy c-medoid \citep{krishnapuram2001fuzzycmedoids}. \textcolor{red}{FONCTIONNEMENT DE CES DEUX ALGO}. Le principe des algorithmes de clustering flou consiste en deux étapes principales \citep{sabzi2011fuzzykmedoids}: 

\begin{enumerate}
 \item l'estimation des degrés d'appartenance de chaque instance $x_i \in X$ à chaque cluster $j \in [1..K]$ de centroïde $z_j$ réalisée par la minimisation de la fonction objective $P(X,Z) = \sum\limits_{i=1}^{n}\sum\limits_{j=1}^{k} \left[u_{ij}r(x_i,z_j)\right]$ \citep{krishnapuram2001fuzzycmedoids}  améliorée par \citet{sabzi2011fuzzykmedoids} en:
 \[P(X,Z) = \sum\limits_{i=1}^{n}\sum\limits_{j=1}^{K} \left[u_{ij}r(x_i,z_j)\right] + \lambda \sum\limits_{i=1}^{n}\sum\limits_{j=1}^{K} \left[ u_{ij}\log_2(u_{ij}) \right] \]
 \[s.c. \sum\limits_{j=1}^{k} u_{ij} = 1\]
 \[0 \leq u_{ij} < 1\]
 
 dont la valeur approximative généralement utilisée de la solution est \[u_{ij} = \frac{\exp\left(\frac{-r(x_i,z_j)}{\lambda}\right)}{\sum_{l=1}^{k}\exp\left(\frac{-r(x_l,z_j)}{\lambda}\right)},\] $r(x_i,z_j)$ étant la mesure de dis-similarité (distance) entre $x_i$ et  $z_j$;
 \item la détermination des nouveaux centres de clusters qui s'effectue toujours par la moyenne des membres du cluster chez le fuzzy c-means, mais par le choix de l'objet $x_q$ qui optimise la somme des distances de cet objet aux autres membres pondérée chacune par le degré d'appartenance de ces autres membres: \[\forall j \in \left[ \left[1;k \right] \right], q = \argmin\limits_{1 \leq l < s_j} \sum\limits_{l=1}^{s_j} \left[u_{lj}r(x_l,z_j)\right]\] $s_j$ étant le nombre de membres du cluster $j$.
\end{enumerate}
 Ainsi l'objectif de l'entrainement des algorithmes de clustering flou est double: déterminer les valeurs optimales du vecteur  $U$ des degrés d'appartenance et de l'ensemble $Z$ des centroïdes. 
 
 Les regroupements avec chevauchement sont intéressants dans notre cas parce qu'il n'est pas exclu qu'une décision traite de plusieurs circonstances factuelles.
\textcolor{red}{A COMPLETER!!!!!!!!!!}
%Pour des regroupements hiérarchiques, des algorithmes comme celui du clustering par agglomération (\textit{agglomeration clustering}) sont mieux indiqués. Le principe du clustering par agglomération est de ...
%si les chevauchements sont négligeables ou n'existent pas, ou bien si la structure hiérarchique permettrait de mieux expliquer et distinguer les différences inter-groupes et les ressemblances intra-groupes. Nous souhaitons organiser des décisions de justices en fonction des circonstances factuelles auxquelles ces documents sont liés.  On pourrait par exemple faire une restriction des données aux cas où chaque document n’appartient qu’à une classe et proposer un système de clustering disjoint.

\subsubsection{Limites des algorithmes de clustering}
nombre prédéfini de clusters, initialisation aléatoire des centroïdes menant à des clusters différents entre plusieurs exécution \citep{sabzi2011fuzzykmedoids}. Nous noterons aussi la dépendance à la métrique de similarité.
%Algorithme kmeans + kmédoids pour les documents: https://pdfs.semanticscholar.org/a46f/efdb64a01d1e6390c8212d881b9c4414ffbf.pdf


\subsection{Métriques de similarité ou de dis-similarité}
\label{sec:similarite:metriques-de-sim}
Une métrique de (dis)similarité est une fonction réelle d'une paire de éléments $x$ et $x'$ d'un ensemble $\mathcal{X}$. Une métrique de dis-similarité mesure le degré de différence entre $x$ et $x'$  généralement estimée par une fonction distance $Dis$  qui satisfait aux propriétés suivantes $\forall x,x',x'' \in \mathcal{X}$ \citep{wang2015distancemetriclearningsurvey}:
\begin{enumerate}
\item $Dis(x,x') \geq 0$ ("non-négativité")
\item $Dis(x,x') = 0  \Leftrightarrow x = y$ (identité discernable)
\item $Dis(x,x') = Dis(x', x)$ (symétrie)
\item $Dis(x,x'') \leq Dis(x,x') + Dis(x',x'')$ (inégalité triangulaire) \label{enum:sim:ineq-tri}
\end{enumerate}


La similarité est donc calculée par $Sim(x,x') = 1 - Dis(x,x')$, la distance représentant la dis-similarité.
%On parle de \textbf{pseudo-métrique} lorsque la condition \ref{enum:sim:ineq-tri} n'est pas satisfaite.

Il existe de nombreuses métriques de similarité généralement expérimentées pour le clustering de textes \citep{huang2008similarityTextClustering}:
\begin{itemize}
	\item les distances de Minkowski de forme générale $Dis(x,x') = \norm{x - x'}_{Lp} = \sqrt[p]{\sum \vert x_i - y_i \vert ^p}$, dont font partie la distance euclidenne ($p=2$) et la distance de Manhattan ($p=1$); 
	\item la distance cosinus: $Dis(x,x') = \sqrt{1 - \frac{x^Tx'}{\norm{x}\norm{x'}}}$
	\item le coefficient similarité de Jaccard: $Sim(x,x') = \frac{x^Tx'}{\norm{x}^2\norm{x}^2 - x^Tx'}$ désignant l'ensemble des termes de $x$ dans le cas où $x$ est un document;
	\item Le coefficient similarité de Dice: $Sim(x,x') = \frac{2\cdot \vert tok(x) \cap tok(x') \vert}{\vert tok(x) \vert + \vert tok(x')} $
	\item Le coefficient de correlation de Pearson: pour $TF_x = \sum\limits^m_{i=1} w_x,i, \forall x \in \mathcal{X}$,
	
	$Sim(x,x') = \frac{m \sum\limits^m_{i=1 w_x,i w_x',i - TF_xTF_x'}}{\sqrt{[m \sum\limits^m_{i=1} w^2_x,i - TF^2_x][m \sum\limits^m_{i=1} w^2_x,i - TF^2_x]}} $;
	\item Distance de la divergence moyenne de Kullback-Leibler $Dis(x,x') = $?
	\item Okapi BM25 est une métrique de similarité généralement utilisée en recherche d'information; les résultats de ce chapitre ont été obtenu avec une version adaptée pour les long documents, la BM25L de \citet{Lv2011BM25L} qui démontrent que la version originale n'est pas robuste lorsque les documents sont longs: \[Sim(x,x') = \sum\limits_{i=1}^n\left(\frac{n_i(k_1 + 1)}{n_i + k_1(1 - b + b\frac{\vert D \vert}{\overline{\vert D \vert}})} \cdot \log \frac{N - n_i + 0.5}{n_i + 0.5}\right)\] \textcolor{red}{corriger avec la formule de BM25L et vérifier la symétrie du BM25}
%	\item TF-IDF ?
%	\item BoW ?
%	\item Componential Counting Grid ?
%	\item mSDA ?
%	\item LDA ?
%	\item LSI ?
	\item \og La distance du déplaceur de mot \fg{} (\textit{word mover's distance - WMD}) \citep{kusner2015wordmoverdist} est une méthode dont l'objectif est similaire au notre, i.e. inclure la similarité sémantique entre les paires de mots de deux documents dans l'estimation de la distance entre ces derniers. En effet, elle est la solution optimale du problème de transport suivant \footnote{Valeur minimale du cout cumulatif pondéré nécessaire pour déplacer  tous les mots de $d$ à $d'$ i.e. transformer $d$ en $d'$.}:
	
	\begin{equation*}
	\begin{aligned}
Dis(d, d') = 	& \min\limits_{T>0}
	& & \sum\limits_{i,j=1}^n T_{ij} c(i,j) \\
	& \text{s.c.}
	& & \sum\limits_{j=1}^n T_{ij} = d_i, \forall i \in {1, \dots, n} \\
	& 
	& & \sum\limits_{i=1}^n T_{ij} = d'_j, \forall j \in {1, \dots, n}	
	\end{aligned}
	\end{equation*} 
	
	$n$ est le nombre de mots considérés; $T$ est une matrice dont $T_{ij}$ est interprété comme étant la quantité du mot $i$ de $d$ qui est va ("voyage") au mot $j$ dans $d'$; $c(i,j)$ est la distance euclidienne entre les vecteurs des mots $i$ et $j$; $d_i$ et $d'_j$ sont les composantes aux mots $i$ et $j$ resp. des vecteurs normalisés sac-de-mots de $d$ et $d'$ reesp. i.e. $d_i = \frac{compte(i, d)}{\sum\limits_{k=1}^n compte(k, d)}$, où $compte(i, d)$ est le nombre d'occurrences du mot $i$ dans $d$.
	
\end{itemize}


Par contre, les métriques {apprises} sont définies à partir de connaissances des données labellisées. Ces métriques sont apprises pour répondre à la difficulté d'identifier la métrique statique appropriée pour un problème. L'apprentissage exploite un corpus préalablement annoté. L'apprentissage peut être supervisé si l'annotation du corpus consiste soit en classifiant des documents\footnote{Organisation des documents d'entraînement en des groupes aux labels prédéfinis.} \citep{weinberger2005LMNN}, soit en affectant des mesures de similarité à des paires de documents \citep{bibid}.  Un apprentissage semi-supervisé typique utilise des données annotées par jugements relatifs sur des pairs ou triplets de documents. Les contraintes de couples consistent en deux ensembles, l'un comprenant des couples de documents qui doivent être similaires, et l'autre contenant des couples de documents dis-similaires. Les contraintes de triplets consistent à définir pour un triplet de documents $(x_1,x_2,x_3)$ une comparaison de degré de similarité entre les paires (par exemple, $x_1$ est plus similaire à $x_2$ qu'à $x_3$). La métrique apprise est néanmoins une véritable métrique à valeur réelle positive écrite sous la forme d'une distance de Mahalanobis $f(x,y) = \sqrt{(x-y)^T M^{-1}(x-y)}$ (où $M$ est la matrice à apprendre). 
 
 L'apprentissage expérimenté dans ce chapitre est supervisé, même s'il utilise des données synthétiques. Nous supposons étant donné que les documents du corpus à \textit{clusteriser} sont tous de la même catégorie de demande, la différence entre les clusters et leur homogénéité se remarquera au niveau des faits. Par cette hypothèse, il reste un risque que d'autres types de regroupements se forment comme par exemple suivant d'autres catégories de demande co-occurrentes. Parmi les divers algorithmes réalisant un apprentissage supervisé, notons par exemple:
 \begin{itemize}
 	\item Les plus-proches-voisins-dans-la-large-marge (LMNN) \citep{weinberger2005LMNN} plus adapté à l'annotation par classification;
 	\item L'analyse des composants du voisinage (NCA) \citep{goldberger2005NCA};
 	\item L'apprentissage de métrique pour la régression noyau (\textit{MLKR}) \citep{weinberger2007MLKR};
 	\item L'analyse discriminante locale de Fisher (LFDA) \citep{sugiyama2007LFDA, } méthode supervisée (données labellisées) de réduction de dimension
 \end{itemize}


\subsection{Déterminer le nombre approprié de clusters (validation)}

\textcolor{red}{faire un tableau des indices comme dans l'article, et comparer les combinaison indices-algo-distance}

 Au delà de l’algorithme à utiliser, le nombre $K$ approprié de clusters doit être déterminé mais pas prédéfini, puisqu'il est difficile de savoir à l'avance le nombre de groupes, le clustering permettant de proposer automatiquement un regroupement. . Une méthode très connue est celle du \og coude \fg{}  (ou \og genou \fg{}) \citep{halkidi2001clustvalidation}, qui est basé sur le principe de base des algorithmes de partitionnement (e.g. K-moyennes) i.e. minimiser le critère d'inertie\footnote{la variance intra-cluster qui est la somme au carré des erreurs (distance d'un membre au centre).}.
\[J(K) = \sum\limits_{j=1}^K\sum\limits_{x_i \in C_j}\norm{x_i-\overline{x_j}}^2\]

$C_j$: ensemble des objets du cluster $j$

$\overline{x_j}$: échantillons moyens du cluster $j$

La méthode du coude consiste à essayer différentes valeurs consécutive de $K$ (de $K_{min}$ à $K_{max}$) puis de choisir celle qui correspond au coude de la courbe du critère d'inertie $J(K)$. Le choix de ce coude est visuel et peut être ambigu (plusieurs valeurs de $K$ sur le coude par exemple). 

La méthode de la silhouette moyenne \citep{rousseeuw1987silhouetteclusternumber} est une alternative qui consiste à choisir comme valeur optimale de $K$, celle qui maximise le critère de la largeur moyenne de la silhouette: $S(k) = \frac{1}{K}\sum\limits_{i=1}^N s(d_i)$. La largeur $s(d_i)$ de la silhouette est un indice qui compare la ressemblance d'un document $d_i$ aux autres membres de son cluster $C_t$ par rapport à ceux d'autres clusters $C_l, l \neq t$:
\[s(d_i) = \frac{b(d_i) - a(d_i)}{\max\lbrace a(d_i),b(d_i)\rbrace}\]

où $a(d_i) = \frac{1}{\vert C_t \vert} \sum\limits_{j=1}^{\vert C_t \vert} Dis(d_i, d_j)$, et $b(d_i) = \min\limits_{l \neq t} \frac{1}{\vert C_l \vert} \sum\limits_{j=1}^{\vert C_l \vert} Dis(d_i, d_j)$.

$K$ a une valeur optimale lorsque la largeur moyenne $S(k)$ atteint sa valeur maximale.

Salvador et Chan (2004) propose d’utiliser l’intersection des deux lignes approximant la courbe. Mais plus récemment, Zhang et al. (2016) trouvent que cette approche n’est pas appropriée pour les cas où le graphe d’évaluation n’est ni lisse, ni monotone. Ils proposent d’exploiter la courbure du graphe i.e. la valeur dont un objet géométrique s'écarte d'être plat ou droit dans le cas d'une ligne.

Etant donné le grand nombre de méthodes existantes \citep{liu2010interclustvalidation, Amorim2015recoveringnumclust}, la majorité des votes peut être appliquée pour choisir le bon $K$ \footnote{\url{https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/}}.

\subsection{Initialisation des centroïdes}

\subsection{Définir une représentation appropriée pour les textes}
\url{https://arxiv.org/pdf/1509.01626.pdf}

\url{http://ad-publications.informatik.uni-freiburg.de/theses/Bachelor_Jon_Ezeiza_2017_presentation.pdf}


\subsection{Labeliser les clusters}

\subsection{Evaluation du clustering généré}
L'évaluation des résultats peut être supervisée ou non selon que l'on dispose ou pas respectivement d'exemples de données annotés avec les groupes attendus.

\subsubsection{Métriques supervisées}
\label{sec:similarite:biblio:supeval}
Même s'il existe un très grand nombre de mesures d'évaluation de la qualité du clustering \citep{im2003clusteringsurvey}, très peu sont couramment utilisées. Il s'agit pour l'évaluation supervisée de l'information mutuelle normalisée (NMI) \citep{}, l'indice ajusté de Rand (\textit{ajusted Rand index} - ARI) \citep{}, et la précision du clustering (ACC) \citep{}. Ces métriques doivent être utilisées ensemble  pour compenser les limites de chacune d'elles \citep{yang2017kmeansfriendlyspaces}. Pour l'évaluation non-supervisée, la pureté, l'erreur et la silhouette sont les métriques généralement utilisées.

\subsubsection{Métriques non-supervisées}
\label{sec:similarite:biblio:unsupeval}



\section{Méthodes proposées}

\subsection{K-médoïdes et \og Word Mover's Distance \fg}

Les approches de clustering sont généralement appliquées à une représentation vectorielle des objets. Particulièrement la méthodes des K-moyennes qui met à jour le centroïde en faisant la moyenne des menbres de son cluster. Cependant, \citet{kusner2015wordmoverdist} ont proposé récemment \textit{la distance du déplaceur de mot} (\textit{word mover's distance - WMD}), une métrique non-supervisée qui permet à la méthode des \textit{K plus proches voisins} (KNN) d'obtenir des performances sans précédents. De plus, l'algorithme de clustering K-médoïdes \citep{kaufman1987kmedoids}, similaire aux K-moyennes, choisi comme centroïde le membre du cluster qui minimise la distance aux restes des membres; ce qui n'impose pas de représentation vectorielle. Ainsi, nous pouvons utiliser la métrique WMD dans  l'algorithme des K-médoïdes. Tout en nous appuyant sur un algorithme établi de clustering, nous évitons aussi la recherche de la meilleure représentation vectorielle qui influence souvent les performances du clustering. 

Algorithme: \url{http://isiarticles.com/bundles/Article/pre/pdf/79087.pdf}


Un des désavantage de l'algorithme des K-médoïdes est son long temps de calcul dû à ???. Nous avons, par conséquent, remplacé la distance euclidienne par la WMD dans la version plus rapide de \cite{Park2009fastkmedoids} avec nombre de clusters prédéfinis, et celle de \cite{sabzi2011fuzzykmedoids} qui intègre une optimisation du nombre de clusters.
%\section{Méthode 2: cartes auto-organisatrice de Kohonen et concaténation de plongement sémantique de phrases (sentence embedding)}

\subsection{Apprentissage d'une métrique fondée la modification du document}
Les algorithmes de clustering et de classification s'appuient généralement sur une représentation vectorielle à partir de laquelle une valeur de similarité est calculée de manière non supervisée et avec une formule mathématique statique. Dans le cas des textes, Il n'est pas toujours évident de choisir la représentation adaptée à une sémantique précise et objective voulue. De plus, il existe un large éventail de schémas de représentations vectorielles. Elles vont des représentations statistiques comme TF-IDF, aux représentations apprises comme le Doc2Vec ou Sent2Vec, et en passant par les agrégations pondérées de modèles distribués de mots (Word2Vec par ex.). 

L'idée dans notre approche est de proposée une formulation de la similarité entre deux documents qui est basée les modifications nécessaires pour transformer un document en un autre. Cette fonction  est apprise à partir d'une base synthétique d'entraînement de similarité entre paires de textes. 
\begin{postulat}
La distance entre deux documents est une fonction des perturbations (modifications) permettant de transformer un document en l'autre. \label{post:similarite:distance}
\end{postulat}
La similarité est définie en fonction de la notion de perturbation du contenu d'un texte (Postulat \ref{post:similarite:distance}): après une légère perturbation, le sens d'un texte reste assez similaire à celui de l'original; et après un grand nombre de modification, le sens du  texte sera très différent de l'original. La similarité sémantique entre des textes décroît donc avec l'intensité des modifications permettant de passer d'un document à un autre. 

La distance sémantique étant une valeur continue, en entrainant un modèle de régression sur un ensemble de paires de documents pour lesquelles on connait la distance, il est possible de la prédire pour de nouvelles paires. Nous proposons de générer une base synthétique de paires de documents dont l'un est un texte original mais l'autre est le résultat de substitutions et suppression de mots dans le premier. Etant donné, que nous contrôlons ces modifications il est plus facile d'affecter une distance qui y corresponde.

\subsubsection{Génération d'une base synthétique d'apprentissage}
Nous définissons une densité de probabilité de perturbation $P \in [0; 1]$ associée à la probabilité de modifier un mot (suppression / remplacement par un mot très différent). Considérons que pour tout texte $x$, $W_x$ désigne l'ensemble des mots dans $x$.
Nous définissons un patron ou une famille de métriques:
\begin{equation}
\begin{array}{cccccc}
Dis & : & C \times C & \to & \mathbb{R} & \\
 & & x, y & \mapsto & Dis(x, y) & = f(P_{x,y}) \\
\end{array}
\end{equation}

$C$ est le corpus. $P_{x,y}$ est l'ensemble des modifications de $x$ nécessaire pour obtenir $y$ i.e. les paires $(w_x, w_y)$ telles que $w_x \in W_x$ a été remplacé par $w_y \in W_y$. $d$ désigne la métrique. Un simple estimateur de $d$ peut être de considérer le taux de mots modifiés dans $x$: 
\begin{equation}
\widetilde{Dis}(x,y) = \frac{\norm{P_{x,y} }}{\norm{W_x}}
\end{equation}
 Un tel modèle ne considère ni l'ordre des mots, ni celui des phrases, la complexité de la modification (une substitution est plus complexe qu'une suppression ou une insertion), ni la différence sémantique entre les mots substituants et les remplacés. Ce dernier aspect peut être estimé en lissant le taux de perturbation à l'aide de la distance sémantique entre les mots substitués (le vecteur représentant le mot vide étant le vecteur nul par ex.):
\begin{equation}
\widetilde{Dis}(x,y) = \frac{\sum\limits_{(w_x, w_y) \in P_{x,y}} Dis_{cos}(w_x,w_y)}{\norm{W_x}} \label{equation:similarite:somme-dist-mots}
\end{equation}
$d_w$ désignant la distance sémantique entre les mots. Ainsi, les substitutions sont pondérées par la distance cosinus entre les vecteurs des mots échangés.

Il est difficile de calculer de telles distances sur un grand corpus étant donné la longueur de notre document. 
Nous ne pouvons que l'estimer. Pour cela, nous générons un jeu artificiel de données pour l'entraînement d'un modèle régressif d'estimation de la métrique entre deux textes $x$ et $y$. En effet, nous partons d'un ensemble $C$ de documents et pour chacun de ces documents, noté $x$, nous générons aléatoirement une valeur seuil de probabilité de perturbation en deça duquel un mot de $x$ est modifié. Par la suite, le texte  $y$, résultant de la modification de $x$, est généré en modifiant les mots de $x$ :

\begin{algorithm}[!htb] % Version française avec : https://pierre.chachatelier.fr/latex/index.php
 \KwData{texte $x$, valeur seuil de probabilité $p$}
 \KwResult{$x'$, $\widetilde{Dis}(x,x')$}
 ${x'} = [] $\; 
 $P_{x,x'} = \emptyset$\;
 \For{$w_x$ in $x$}{
 	$v = random(0,1)$\;
    \eIf{v < p}{
       $w_x' += modifie(w_x)$; // Algorithme \ref{algo:similarite:modifiemot} \;
       ${x'} += w_{x'}$ \;
       $P_{x,x'} = P_{x,x'} \cup \lbrace (w_x, w_{x'}) \rbrace$\;
     }{
     $x' += w_x$\;
     }
     $\widetilde{Dis}(x,x') = \frac{\sum\limits_{(w_x, w_{x'}) \in P_{x,x'}} Dis_{cos}(w_x,w_{x'})}{card(W_x)}$\;
 }
 \Return $x', \widetilde{Dis}(x,x')$\;
 \caption{Génère une perturbation $x'$ de $x$} \label{algo:similarite:perturbation}
\end{algorithm}


\begin{algorithm}[H]
 \KwData{un mot $w$, le vocabulaire $W$ }
 \KwResult{un mot $w'$} 
 $w' = $ un mot différent de $w$ choisi aléatoirement dans $W$\;
 \Return $w'$
 \caption{Modifie un mot donné $w$} \label{algo:similarite:modifiemot}
\end{algorithm}

Pour garantir la symétrie de la métrique, nous imposons $\widetilde{Dis}(x', x) = \widetilde{Dis}(x, x'), \forall x \in C$. Pour garantir la réflexivité, nous imposons $\widetilde{Dis}(x, x) = 0, \forall x \in C$.

\subsubsection{Apprentissage de la métrique}

Sur l'ensemble $B = \lbrace (d_i, d'_i, \widetilde{Dis}(d_i, d'_i))\rbrace_{1 \leq i \leq \norm{B}}$ de données synthétiques d'entraînement, on entraîne un modèle régressif $m$ pour prédire la similarité entre 2 documents en fonction de leur représentation vectorielle. Ce modèle régressif peut être utilisé comme métrique de similarité dans un algorithme de clustering comme l'algorithme des K-moyennes au même titre que les métriques citées à la Section \ref{sec:similarite:metriques-de-sim}. Cependant, les modèles de régression ne supportent généralement qu'un seul vecteur en entrée, et pas deux vecteurs comme en dispose la base $B$. Nous somme donc amené à définir l'agrégation adéquate des vecteurs $d_i$ et $d'_i$. L'agrégation qui fonctionne le mieux est la soustraction avec laquelle les documents similaires auront une agrégation avec un grand nombre de composantes tendant vers 0. La fonction d'estimation automatique de la distance sémantique entre $x$ et $y$ s'écrit: $Dis(x, y) = m(x-y)$. 

%\textcolor{red}{Issues:}
%\begin{itemize}
%\item les docs sont généralement de tailles différentes, ne faudrait il pas intégrer une perturbation ajout de mots? \textcolor{blue}{la suppression peut être considérée comme le remplacement d'un mot par le mot vide, qui doit être ajouté aux word2vec}
%\item il faudrait intégrer la composante taille du document: \textcolor{blue}{agréger sur le nombre minimal de phrases des paires de documents}
%\item comment assurer les propriétés d'une fonction similarité? par exemple si aucune perturbation n'est opérée, alors la similarité est maximale et si tous les mots sont modifiés alors la similarité est minimale: \textcolor{blue}{agrégation par soustraction des vecteurs du couple de docs. plus deux doc seront similaire, plus le vecteur de leur paire tendra vers le nul}
%\item Ne faudrait il pas prendre en compte un poids pour les mots, car peut-être la modification de certains mots ne devrait pas avoir le même impact sur la similarité ou le taux de perturbation que celle d'autres mots:  \textcolor{blue}{lissage par la somme des distance des vecteurs de mots substitués Eq. \ref{equation:similarite:somme-dist-mots}}
%\item ne faudrait il pas intégré une métrique proche de la tâche: la ressemblance n'est pas forcément globale à tous le corps du document mais plus à certaines régions; donc un document auquel on rajoute quelques phrases ne devrait pas voir  son sens trop changer:  \textcolor{blue}{peut-être agréger les distances minimales entre les paires de phrases}
%\end{itemize}

\section{Expérimentations et interprétation des résultats}
\label{sec:similarite:experimentations}

\subsection{Configuration}

\subsection{Annotations de données d'évaluation}
Pour l'évaluation supervisée, nous disposons d'une base annotée sur la catégorie de demande "dommage-intérêts / action en responsabilité civile professionnelle contre les avocats" qui concerne les contentieux impliquant des avocats.
L'expert annotateur a identifié quatre cas différents (a, b, c, d) décrits en annexe. En gros:
\begin{itemize}
\item pour le cas a) il s'agit d'un avocat qui est négligent et envoie son assignation de manière tardive (champ sémantique: retard/délai/prescription)
\item pour le cas b) il s'agit d'un avocat qui n'a pas donné un conseil opportun, qui n'a pas soulevé le bon argument
\item pour le cas c) un avocat qui n'a pas rédigé un acte valide ou réussi à obtenir un avantage fiscal (champ sémantique: rédacteur d'actes)
\item pour le cas d) il s'agit d'un avocat attaqué par son adversaire et non pas par son propre client.
\end{itemize}

Le dataset comprend 81 documents répartis dans 4 groupes avec 6 documents appartenant chacun à 2 groupes.

Pour l'évaluation non supervisée, les 6 catégories de demande utilisée pour l'extraction de demandes sont utilisés en plus.

\subsubsection{Prétraitement}
Suppression des \textit{stop words} car il sont généralement indépendant de toute catégorie

\subsection{Apprentissage de la métrique}
Matrices de (dis)similarité pour chaque distance

\subsection{Comparaison d'approches}
Comparer la vectorisation du document sur tout son contenu vs. sur la restriction aux énoncés de demande de la catégorie (du type "constater", "dire et juger") vs restriction aux conclusions (le raisonnement des parties décrits les circonstances factuelles) + motifs sur la catégorie

avec choix du nombre de clusters


\section{Conclusion}
\label{sec:similarite:conclusion}
jhk
lk

lkjkl