 \chapter{Modélisation des circonstances factuelles par catégorisation non supervisé de documents}
\label{chap:similarite}

\section{Introduction}
\label{sec:similarite:introduction}
Les circonstances factuelles définissent les contextes possibles dans lesquels une catégorie de demande peut être formulée. Les analyses descriptives ou prédictives ne prennent sens que lorsqu'elles sont appliquées à un ensemble de décisions aux circonstances similaires. Par exemple, il serait imprudent de considérer toutes les décisions pour analyser les chances d'acceptation d'une demande de dommages-intérêts fondée sur l'\og article 700 du code de procédure civile \fg{} en cas de trouble de voisinage. Les taux d'acceptation ou de rejet peuvent être différents entre des affaires de licenciement et celles portant sur les troubles anormaux du voisinage, et même plus spécifiquement entre les troubles de voisinage entre particulier et entreprises. % (par exemple: chantier de construction), ou simplement entre particuliers (par exemple: troubles sonores). 
 Il serait préférable de travailler uniquement avec des décisions similaires à la situation d'intérêt. L'identification des circonstances factuelles devient donc une étape préalable indispensable à l'analyse du résultat. Malheureusement, les circonstances sont très diverses et quasi infinies pour être identifier par classification supervisée à l'aide d'annotation manuelle d'exemples comme dans les chapitres précédent. Il est donc plus indiquer d'adopter une approche non-supervisée capable modéliser les circonstances factuelles à partir d'un corpus de documents d'une même catégorie de demande. Plus précisément, la méthode doit construire des sous-ensembles de décisions selon qu'elles traitent de contextes similaires.  Les objectifs de ce chapitre sont d'expérimenter des algorithmes  de catégorisation (\textit{clustering}) et des métriques de similarité. Il démontre aussi qu'une distance entraînée  sur des documents d'une catégorie de demande permet de mieux mesurer la (dis-) similarité sémantique définie par les circonstances factuelles.

%\section{Formulation du Problème}
%\label{sec:similarite:probleme}

\section{Catégorisation non-supervisé de documents}
\label{sec:similarite:biblio}

Cette section fait une synthèse bibliographique des différents aspects rentrant dans la conception d'un système de catégorisation de documents. Elle aborde principalement le choix de l'algorithme, la définition d'une mesure de similarité adéquate, la représentation des documents, la détermination du nombre de {clusters}, et l'évaluation de la catégorisation générée.

L'ensemble des $N$ documents est noté $\mathcal{D}$. Tout document $d \in \mathcal{D}$ est une séquence de mots $d=(d[1], d[2], \dots, d[\setsize{d}])$, où $d_i$ est le mot à la position $i$ dans $d$. Sa représentation vectorielle est notée $\vec{d}=(\vec{d}[1], \vec{d}[2], \dots, \vec{d}[m])$. Pour un modèle vectoriel TF-IDF de vocabulaire $T = \lbrace t_1, t_2, \dots, t_m \rbrace$, $\vec{d}_i = w(t_i,d)$ est le poids du terme $t_i \in T$ dans le texte $d$ ({cf. \ref{sec:quanta:classification}}). La catégorisation obtenue est un ensemble de clusters $C = \lbrace C_1, C_2, \cdots, C_K \rbrace$, $K$ étant le nombre de clusters formés.

\subsection{Algorithmes de catégorisation non-supervisé}

La catégorisation de documents a pour objectif  d'identifier, sans supervision\footnote{Sans utiliser des exemples annotés.}, une structure pertinente (pour le domaine expert) dans l'ensemble $\mathcal{D}$ en construisant des groupes représentants des catégories inconnues au départ. Ces groupes, appelés clusters, peuvent être disjoints ou se chevaucher, et plates ou hiérarchiques suivant les contraintes du domaine expert. L’algorithme à utiliser dépend généralement de la forme qu’on souhaite donner à l’organisation. 

\subsubsection{Partitionnement disjoint}
Pour réaliser des partitions distinctes\footnote{Chaque document n'appartient qu'à un seul cluster.} (\textit{hard clustering}), des algorithmes tels que celui des K-moyennes \citep{forgey1965kmeans} et celui des \textit{K-medoïdes} \citep{kaufman1987kmedoids} seront préférés \citep{balabantaray2015kmeanskmedoids}. Ces deux algorithmes fonctionnent de manière similaire, et nécessitent que le nombre $K$ de clusters soient prédéfini. Ils commencent par une définition aléatoire de $K$ centres initiaux de clusters (centroïdes) et l'affectation des différents documents au cluster dont le centre est le plus proche. S'en suit une boucle dans laquelle le centroïde est recalculé (le point à distance totale minimale avec les membres du cluster) et les documents sont réaffectés chacun au cluster dont le centroïde est le plus proche. L'algorithme s'arrête si aucune amélioration n'est plus observée, ce qui se traduit soit par l'atteinte d'une valeur minimale prédéfinie de l'erreur de catégorisation\footnote{Somme des distances au carré entre les points et leur centre respectif.} ou d'une mesure d'évaluation non supervisée (\ref{sec:similarite:biblio:unsupeval}). La différence entre l'algorithmes des K-moyennes et celui des \textit{K-medoïdes} tient principalement au fait que les centroïdes du premier ne sont pas nécessairement des points (documents) de l'ensemble d'origine, mais des points moyennes des représentations vectorielles des membres du cluster, contrairement à l'algorithme des \textit{K-medoïdes} qui ne considère que les documents originaux qui ont une distance minimale à tous les documents dans leur cluster. Cette différence donne l'avantage au \textit{K-medoïdes} de ne pas dépendre d'une représentation vectorielle nécessaire au calcul de la moyenne, mais elle a aussi l'inconvénient d'augmenter sa complexité en temps  car il faut calculer et stocker la distance entre toutes les paires de documents. Il existe plusieurs autres algorithmes de catégorisation disjointe dont le principe de fonctionnement est différent de celui des K-moyennes. Par exemple, l'algorithme DBSCAN (\textit{Density-based spatial clustering of applications with noise}) \citep{ester1996dbscan}  ne prend pas en paramètre le nombre de clusters à construire. Il est défini sur le concept de régions de densité caractérisées par la distance minimale $\epsilon$ autorisée entre deux points d'une même région, et le nombre maximal de points qui doivent être dans le voisinage de rayon $\epsilon$ d'un point pour que ce voisinage soit une région de densité (le point central est appelé "point noyau" (\textit{core point}). Le principe du DBSCAN est de construire les clusters successivement en reliant les régions (voisinages) dont les noyaux sont à distance plus ou moins inférieure à $\epsilon$. Les points qui sont seul dans leur cluster sont qualifiés d'\textit{outliers}. 
% amélioration par réduction de dimension
En outre, la catégorisation spectrale est une autre méthode efficace de catégorisation qui effectue préalablement une réduction de dimensions à l'aide du spectre de la matrice de similarité $M \in \mathbb{R}^{N \times N}$ \footnote{$M_{ij}$ est la mesure de la similarité entre les points (documents) $d_i$ et $d_j$ du corpus $D$.} des données  avant d'appliquer un algorithme traditionnel comme celui des K-moyennes. Les dimensions du nouvel espace sont définies par les vecteurs propres de la matrice Laplacienne $L$ de $M$ \citep{shi2000spectralClustering, von2007tutorialSpectralClustering} qui peut être normalisée ($L = T^{-1/2}(T-S)T{-1/2}$) ou pas ($L = T - M$), $T$ étant la matrice diagonale déduite de $M$ i.e. $T_{ii} = \sum\limits_j M_{ij}$. 

Il est aussi possible d'utiliser les arbres de décision pour améliorer les résultats des K-moyennes. En effet, les forêts aléatoires \citep{breiman2001randomforest} permettent d'estimer la similarité entre deux points. Le principe consiste à générer un ensemble de $n$ points synthétiques, et d'entraîner une forêt aléatoire à une classification binaire supervisée avec les points originaux considérés dans la classe des "originaux" et les données synthétiques dans la seconde classe des "synthétiques" \citep{afanador2016unsupervisedrandomforest}. Une forêt aléatoire étant un ensemble d'arbres de décision (classification) construit sur des parties de l'ensemble d'apprentissage duquel on a retiré une ou plusieurs variables prédictives, la similarité entre 2 points est la proportion d'arbres dans lesquels ces points se trouvent dans le même nœud feuille. Cette métrique "apprise" peut-être par la suite utilisée dans un algorithme de catégorisation classique comme les K-moyennes.

\subsubsection{Catégorisation hiérarchique}
La catégorisation hiérarchique consiste à construire une hiérarchie de clusters. Le regroupement hiérarchique ascendant est une technique de catégorisation hiérarchique qui part d'autant de clusters que de documents, chacun des groupe comprenant un élément. Ensuite, l'algorithme détecte et fusionne successivement des sous groupes dont la distance est la plus petite et/ou proche d'une valeur seuil donnée, jusqu'à ce que tous les documents soit dans un unique groupe (racine). Pour déterminer le partitionnement optimal, Le nombre de clusters doit être déterminé. \textcolor{red}{Une technique pour y arriver est de ...\footnote{\url{http://eric.univ-lyon2.fr/~ricco/cours/slides/en/cah.pdf}}}.

%\textcolor{red}{Random Forest - processus de construction: \url{https://onlinelibrary.wiley.com/doi/pdf/10.1002/cem.2790}}

%L'application de ces différents algorithmes aux documents n'est généralement basé que sur les statistiques d'occurrence des termes, et par conséquent les thématiques abordées dans les documents ne sont pas bien prise en compte, surtout que l'élimination des \og mots vides \fg{} (\textit{stop words}) peut laisser les deux documents sans sinon très peu de mots en commun \cite{kusner2015wordmoverdist}. \citet{xie2013MGCTM} démontrent empiriquement que l'intégration de la  modélisation thématique (\textit{topic modeling}) au clustering de documents améliore significativement les résultats. Cette intégration des modèles thématiques dans le clustering de documents peut être réalisée de multiples façons, mais deux méthodes semblent être les plus efficaces:
%\begin{enumerate}
%	\item l'intégration naïve \citep{lu2011kmeansLDApLSA} qui consiste à inférer $K$ thèmes à l'aide d'un algorithme comme le PLSA (aAnalyse sémantique probabiliste latente) \citep{hofmann1999PLSA} ou le LDA (allocation de Dirichlet latente)\citep{blei2003lda}, puis de considérer pour chaque document le thème $j \in [1..K]$ qui a la probabilité $\theta_j$ la plus élevé dans ce document suivant la distribution $\theta$ de probabilité des thèmes dans ce document; le thème choisi $j$ représente le cluster du document;
%	\item le modèle thématique multi-grain de clustering (\textit{multi-grain clustering topic model}) ou MGCTM proposé par \citet{xie2013MGCTM}, dont l'objectif est d'inférer de manière jointe les clusters et le modèle thématique.
%\end{enumerate}

\subsubsection{Catégorisation avec chevauchements}

Lorsque des chevauchements sont observables entre clusters (un document peut faire partie de plusieurs groupes à la fois), chaque objet peut être affecté partiellement à chaque cluster grâce à la notion de degré d'appartenance (\textit{membership degree}) entre un point $x_i \in X$ et le cluster $j \in [1..K]$ estimé par une fonction $u_{ij}$  \citep{baraldi1999surveyfuzzyclstering}. Il est par conséquent préférable d'employer des algorithmes de partitionnement "mou" comme l'algorithme des c-moyennes flou (FCM) \citep{bezdek1984fcm, hathaway1989fuzzycmeans}, ou le fuzzy c-Medoids (FDMdd) \citep{krishnapuram2001fuzzycmedoids}, ou la version améliorée IFKM (\textit{improved fuzzy K-medoids})\citep{sabzi2011fuzzykmedoids}.  Le principe de ces algorithmes consiste en deux étapes principales \citep{sabzi2011fuzzykmedoids}: 

\begin{enumerate}
 \item l'estimation des degrés d'appartenance de chaque instance $x_i \in X$ à chaque cluster $j \in [1..K]$ de centroïde $z_j$ réalisée par la minimisation de la fonction objective $P(X,Z) = \sum\limits_{i=1}^{n}\sum\limits_{j=1}^{k} \left[u_{ij}r(x_i,z_j)\right]$ \citep{krishnapuram2001fuzzycmedoids}  améliorée par \citet{sabzi2011fuzzykmedoids} en:
 \[P(X,Z) = \sum\limits_{i=1}^{n}\sum\limits_{j=1}^{K} \left[u_{ij}r(x_i,z_j)\right] + \lambda \sum\limits_{i=1}^{n}\sum\limits_{j=1}^{K} \left[ u_{ij}\log_2(u_{ij}) \right] \]
 \[s.c. \sum\limits_{j=1}^{k} u_{ij} = 1\]
 \[0 \leq u_{ij} < 1\]
 
 dont la valeur approximative généralement utilisée de la solution est \[u_{ij} = \frac{\exp\left(\frac{-r(x_i,z_j)}{\lambda}\right)}{\sum_{l=1}^{k}\exp\left(\frac{-r(x_l,z_j)}{\lambda}\right)},\] $r(x_i,z_j)$ étant la mesure de dis-similarité (distance) entre $x_i$ et  $z_j$;
 \item la détermination des nouveaux centres de clusters qui s'effectue toujours par la moyenne des membres du cluster chez le fuzzy c-means, mais par le choix de l'objet $x_q$ qui optimise la somme des distances de cet objet aux autres membres pondérée chacune par le degré d'appartenance de ces autres membres: \[\forall j \in  \left[1\twodots k \right], q = \argmin\limits_{1 \leq l < s_j} \sum\limits_{l=1}^{s_j} \left[u_{lj}r(x_l,z_j)\right]\] $s_j$ étant le nombre de membres du cluster $j$.
\end{enumerate}
 Ainsi l'objectif de l'entraînement des algorithmes de catégorisation floue est double: déterminer les valeurs optimales de la matrice $U$ des degrés d'appartenance et l'ensemble $Z$ des centroïdes. 
 
 \citet{nefti2004probabilisticFuzzyCMeans} proposent les C-moyennes floues probabilistes qui sont une variante du FCM pour laquelle la somme des degrés d'appartenance  d'un document aux clusters est de 1.
 
 Les catégorisations avec chevauchement sont intéressants parce qu'il est possible qu'une décision traite de plusieurs circonstances factuelles.
%\textcolor{red}{A COMPLETER!!!!!!!!!!}
%Pour des regroupements hiérarchiques, des algorithmes comme celui du clustering par agglomération (\textit{agglomeration clustering}) sont mieux indiqués. Le principe du clustering par agglomération est de ...
%si les chevauchements sont négligeables ou n'existent pas, ou bien si la structure hiérarchique permettrait de mieux expliquer et distinguer les différences inter-groupes et les ressemblances intra-groupes. Nous souhaitons organiser des décisions de justices en fonction des circonstances factuelles auxquelles ces documents sont liés.  On pourrait par exemple faire une restriction des données aux cas où chaque document n’appartient qu’à une classe et proposer un système de clustering disjoint.

%\subsubsection{Limites des algorithmes de clustering}
%nombre prédéfini de clusters, initialisation aléatoire des centroïdes menant à des clusters différents entre plusieurs exécution \citep{sabzi2011fuzzykmedoids}. Nous noterons aussi la dépendance à la métrique de similarité.
%Algorithme kmeans + kmédoids pour les documents: https://pdfs.semanticscholar.org/a46f/efdb64a01d1e6390c8212d881b9c4414ffbf.pdf


\subsection{Métriques de dis-similarité (distances)}
\label{sec:similarite:distances}
Les algorithmes de catégorisation dépendent de la distance utilisée qui doit être bien choisie pour que les catégorisations révèlent au mieux la sémantique visée.
 Une métrique de dis-similarité $Dis$ est une fonction réelle d'une paire $(d,d')$ qui mesure le degré de dis-similarité entre $d$ et $d'$  en satisfaisant aux propriétés suivantes $\forall d,d',d'' \in \mathcal{D}$ \citep{wang2015distancemetriclearningsurvey}:
\begin{enumerate}
\item $Dis(d,d') \geq 0$ ("non-négativité")
\item $Dis(d,d') = 0  \Leftrightarrow d = d'$ (identité discernable)
\item $Dis(d,d') = Dis(d', d)$ (symétrie)
\item $Dis(d,d'') \leq Dis(d,d') + Dis(d',d'')$ (inégalité triangulaire) \label{enum:sim:ineq-tri}
\end{enumerate}


La métrique peut être normalisée ($\forall (d,d') \in \mathcal{D} \times \mathcal{D};  0 \leq Dis(d,d') \leq 1$), à l'instar de la distance basée sur la similarité cosinus normalisée et celle de Jaccard. Dans ce cas, la relation entre la similarité $Sim$ et la dis-similarité $Dis$ est définie par $Sim(d,d') = 1 - Dis(d,d')$.
%On parle de \textbf{pseudo-métrique} lorsque la condition \ref{enum:sim:ineq-tri} n'est pas satisfaite.

 Il existe de nombreuses métriques de similarité généralement expérimentées pour la catégorisation de textes \citep{huang2008similarityTextClustering, vijaymeena2016surveySim, afzali2018SimKmeans}. Voici les quelques métriques de similarité syntaxique explorées dans ce chapitre:
\begin{itemize}
	\item Les distances de Minkowski de forme générale $Dis(d,d') = \norm{\vec{d} - \vec{d'}}_{Lp} = \sqrt[p]{\sum\limits_{i=1}^m \vert \vec{d}_i - \vec{d}_i \vert ^p}$, dont font partie la distance euclidienne $Dis_{euclidienne}$ ($p=2$) et la distance de Manhattan $Dis_{manhattan}$ ($p=1$).
	\item L'indice de Dice définit la similarité entre $d$ et $d'$ comme étant la proportion de termes qui leur sont communs: $Sim_{dice}(d,d') = \frac{2 \setsize{d \cap d'}}{\setsize{d} + \setsize{d'}}$.
	\item La distance de \citet{bray1957distance-braycurtis}: $Dis_{braycurtis}(d,d') = \frac{\sum\limits_{i=1}^m \vert \vec{d}_i - \vec{d'}_i \vert}{\sum\limits_{i=1}^m \vert \vec{d}_i + \vec{d'}_i \vert}$\citep{huang2008similarityTextClustering}.
	\item La similarité cosinus est basée sur la mesure de l'angle entre $\vec{d}$ et $\vec{d'}$ par la formule: $Sim_{cos}(d,d') = \frac{\vec{d}^t\vec{d'}}{\norm{\vec{d}}\norm{\vec{d'}}}$.
	Pour un modèle vectoriel du type TF-IDF, cette formulation considère que tous les termes du vocabulaire $T$ définissant le modèle vectoriel, sont différents et ne partagent aucune relation. \citet{sidorov2014softcosine} la corrigent en proposant la fonction \textit{soft-cosine} en introduisant une matrice $S = {S_{ij}}_{1\leq i,j \leq m}$ de similarité entre  termes: 
	
	$Sim_{soft-cos}(d,d')= \frac{{\vec{d}}^T\cdot S\cdot \vec{d'}}{\sqrt{{\vec{d}}^T\cdot S\cdot \vec{d'}}\cdot \sqrt{\vec{d'}^T\cdot S\cdot \vec{d'}}} = \frac{\sum\limits_{1\leq i,j \leq m}s_{ij}\vec{d}_i\vec{d'}_j}{\sqrt{\sum\limits_{1\leq i,j \leq m}s_{ij}\vec{d}_i\vec{d}_j}\sqrt{\sum\limits_{1\leq i,j \leq m}s_{ij}\vec{d'}_i\vec{d'}_j}}$,
	
	où $S$, la matrice de similarité entre les termes, peut être calculée à partir de n'importe quelle métrique comme la distance d'édition de Levenshtein (similarité lexicale) \citep{sidorov2014softcosine},  la similarité cosinus entre  plongements lexicaux \citep{charlet2017simbow_acl, charlet2017simbow_tal}, ou la similarité WordNet.
	
	La fonction cosinus  étant comprise entre -1 et +1, la distance déduite $Dis_{cos}(d,d') = 1 - Sim_{cos}(d,d')$ est comprise entre 1 et 2.
	
	\item Le coefficient similarité de \cite{jaccard1901similarite-jaccard}: $Sim_{Jaccard}(d,d') = \frac{\vec{d}^T\vec{d'}}{\norm{\vec{d}}^2+\norm{\vec{d'}}^2 - \vec{d}^T\vec{d'}}$ \citep{huang2008similarityTextClustering} qui, étant normée, donne la distance $Dis_{jaccard}(d,d') = 1-Sim(d,d')$.
	%\item Le coefficient similarité de Dice: $Sim_{Dice}(x,x') = \frac{2\cdot \vert tok(x) \cap tok(x') \vert}{\vert tok(x) \vert + \vert tok(x')} $
	\item La similarité basée sur le coefficient de corrélation de Pearson est calculée par \citep{huang2008similarityTextClustering}:
	
	\[Sim_{pearson}(d,d') = \frac{ \sum\limits^m_{i=1} \vec{d}_i \cdot \vec{d'}_i - TF_d\cdot TF_{d'}}{\sqrt{[m \sum\limits^m_{i=1} \vec{d'}_i^2 - TF^2_d][m \sum\limits^m_{i=1} \vec{d'}_i^2 - TF^2_{d'}]}}\]
avec $TF_d = \sum\limits^m_{i=1} \vec{d}_i$. Sa distance est déduite par:

$Dis_{pearson}(d,d') =
\left\{ \begin{array}{ll}
1 - Sim_{pearson}(d,d') & \text{si } Sim_{pearson}(d,d') \geq 0 \\
\vert Sim_{pearson}(d,d') \vert & \text{si } Sim_{pearson}(d,d') < 0.
\end{array}
\right.$
%	\item Distance de la divergence moyenne de Kullback-Leibler considère un document comme une distribution de probabilité de termes, et mesure donc la similarité entre deux distributions: 
%	\[Dis_{avgKL}(x,x') = \sum\limits_{i=1}^m\big(\pi_1 \cdot D(w(t_i,x) \vert\vert w_t) + \pi_2 \cdot D(w(t_i,x') \vert\vert w_t) \big)\]
%	avec $\pi_1 = \frac{w(t_i,x)}{w(t_i,x) + w(t_i,x')}$, $\pi_2 = \frac{w(t_i,x')}{w(t_i,x) + w(t_i,x')}$, $D(a \vert\vert b) = a\cdot  \log_2(\frac{a}{b})$, et $w_t = \pi_1 \cdot w(t_i,x) + \pi_2 \cdot w(t_i,x')$
%	\item Okapi BM25 est une métrique de similarité généralement utilisée en recherche d'information pour calculer un score de pertinence d'un document D par rapport à une requête Q: 
%	\[Sim_{BM25}(Q,D) = \sum\limits_{w \in Q \cap D} \left( \frac{(k_3+1) \cdot c(w, Q)}{k_3 + c(w, Q)} \cdot f(w,D) \cdot \log \frac{N+1}{df(w) + 0.5)}\right),\]
%	\[\text{avec } f(w,D) = \frac{(k_1+1)\cdot c(w,D)}{k_1(1-b+b\frac{\vert D \vert}{avgdl})} = \frac{(k_1+1)\cdot c'(w,D)}{k_1 + c'(w,D)},\] où $c'(w,D) = \frac{c(w,D)}{1-b+b\frac{\vert D \vert}{avgdl} }$. $c'(w,D)$ pouvant approcher 0 pour des documents très longs, \citet{Lv2011BM25L} propose BM25L, une formulation plus robuste à la longueur des documents obtenue en remplaçant $f(w,D)$ par :
%	\[
%	f'(w,D) =
%	\left\{ \begin{array}{ll}
%	\frac{(k_1+1)\cdot (c'(q,D)+\delta)}{k_1+ (c'(w,D) + \delta)} & \text{si } c'(w,D) > 0 \\
%	0 & \text{si } sinon
%	\end{array}
%	\right.
%	\]
	\item \og La distance du déménageur de mot \fg{} (\textit{word mover's distance - WMD}) \citep{kusner2015wordmoverdist} est une méthode dont l'objectif est similaire au notre, i.e. inclure la similarité sémantique entre les paires de mots de deux documents dans l'estimation de la distance entre ces derniers. En effet, elle est la solution optimale du problème de transport suivant \footnote{Valeur minimale du cout cumulatif pondéré nécessaire pour déplacer  tous les mots de $d$ à $d'$ i.e. transformer $d$ en $d'$.}:
	
	\begin{equation*}
	\begin{aligned}
Dis_{wmd}(d, d') = 	& \min\limits_{T>0}
	& & \sum\limits_{i,j=1}^m T_{ij} c(i,j) \\
	& \text{s.c.}
	& & \sum\limits_{j=1}^m T_{ij} = \vec{d}_i, \forall i \in {1, \dots, m} \\
	& 
	& & \sum\limits_{i=1}^m T_{ij} = \vec{d'}_j, \forall j \in {1, \dots, m}	
	\end{aligned}
	\end{equation*} 
	
	$m$ est le nombre de mots considérés; $T$ est une matrice dont $T_{ij}$ est interprété comme étant la quantité du mot $i$ de $d$ qui est va au mot $j$ dans $d'$ ("voyage"). $c(i,j)$ est la distance euclidienne entre les vecteurs des mots $i$ et $j$; $\vec{d}_i$ et $\vec{d'}_j$ sont les composantes aux mots $i$ et $j$ resp. des vecteurs normalisés sac-de-mots de $d$ et $d'$ reesp. i.e. $\vec{d}_i = \frac{compte(i, d)}{\sum\limits_{k=1}^m compte(k, d)}$, où $compte(i, d)$ est le nombre d'occurrences du mot $i$ dans $d$.
	
\end{itemize}


%Par contre, les métriques {apprises} sont définies à partir de connaissances des données labellisées. Ces métriques sont apprises pour répondre à la difficulté d'identifier la métrique statique appropriée pour un problème. L'apprentissage exploite un corpus préalablement annoté. L'apprentissage peut être supervisé si l'annotation du corpus consiste soit en classifiant des documents\footnote{Organisation des documents d'entraînement en des groupes aux labels prédéfinis.} \citep{weinberger2005LMNN}, soit en affectant des mesures de similarité à des paires de documents \citep{bibid}.  Un apprentissage semi-supervisé typique utilise des données annotées par jugements relatifs sur des pairs ou triplets de documents. Les contraintes de couples consistent en deux ensembles, l'un comprenant des couples de documents qui doivent être similaires, et l'autre contenant des couples de documents dis-similaires. Les contraintes de triplets consistent à définir pour un triplet de documents $(x_1,x_2,x_3)$ une comparaison de degré de similarité entre les paires (par exemple, $x_1$ est plus similaire à $x_2$ qu'à $x_3$). La métrique apprise est néanmoins une véritable métrique à valeur réelle positive écrite sous la forme d'une distance de Mahalanobis $f(x,y) = \sqrt{(x-y)^T M^{-1}(x-y)}$ (où $M$ est la matrice à apprendre). 
 
% L'apprentissage expérimenté dans ce chapitre est supervisé, même s'il utilise des données synthétiques. Nous supposons étant donné que les documents du corpus à \textit{clusteriser} sont tous de la même catégorie de demande, la différence entre les clusters et leur homogénéité se remarquera au niveau des faits. Par cette hypothèse, il reste un risque que d'autres types de regroupements se forment comme par exemple suivant d'autres catégories de demande co-occurrentes. Parmi les divers algorithmes réalisant un apprentissage supervisé, notons par exemple:
% \begin{itemize}
% 	\item Les plus-proches-voisins-dans-la-large-marge (LMNN) \citep{weinberger2005LMNN} plus adapté à l'annotation par classification;
% 	\item L'analyse des composants du voisinage (NCA) \citep{goldberger2005NCA};
% 	\item L'apprentissage de métrique pour la régression noyau (\textit{MLKR}) \citep{weinberger2007MLKR};
% 	\item L'analyse discriminante locale de Fisher (LFDA) \citep{sugiyama2007LFDA, } méthode supervisée (données labellisées) de réduction de dimension
% \end{itemize}

%@inproceedings{lemikolov2014word2vec,
%	title={Distributed representations of sentences and documents},
%	author={Le, Quoc and Mikolov, Tomas},
%	booktitle={International Conference on Machine Learning},
%	pages={1188--1196},
%	year={2014}
%}



\subsection{Représentation des textes}
\label{sec:similarite:representation}
\subsubsection{Modèle vectoriel}
La formulation des distances exploite très souvent une représentation vectorielle des textes (cf.  \ref{sec:similarite:distances}). Le Chapitre \ref{chap:quanta} décrit différentes métriques de pondération des termes qui permettent de définir des variantes du modèle TF-IDF. Ces dernières sont explorées dans les expérimentations de ce chapitre. Cependant, il s'agit de modèles basés uniquement sur le lexique des textes, et par conséquence, les distances appliquées sur ces représentations sont correspondent à une similarité plus lexicale que sémantique. Il existe quelques techniques permettant de définir des dimensions par des thématiques transparaissant dans le corpus, et apportant par conséquent plus de sémantique à la représentation vectorielle.

\subsubsection{Réduction de dimension}
\label{sec:similarite:reduction-dimension}
La réduction de dimension a généralement pour but de réduire la représentation de documents i.e. transformer leur vecteur de dimension originel $m$ en un nouveau vecteur de dimension $q << m$. Les méthodes de réduction explorées ici sont toutes non supervisées\footnote{Elles ne prennent en compte aucune classification prédéfinie des documents.} et réalisent une extraction de caractéristiques\footnote{Par opposition aux algorithmes de sélection de caractéristiques comme le BDS et le SFFS expérimentées au Chapitre \ref{chap:structuration}.}. %L'intérêt de la réduction de dimension est multiples: construire des dimensions plus discriminantes, limiter le sur-apprentissage, et obtenir une représentation sémantique plus concise. 

\paragraph[PCA]{Analyse en composantes principales (ACP)}
L'analyse en composantes principales \citep{burrows1992PCA} est une technique de transformation de l'espace originel en un espace de dimension réduite ($q < m$)  préservant au mieux l'inertie du nuage originel\footnote{Distance entre les individus pris 2 à 2, ou dispersion autour du barycentre}. L'intérêt de l'ACP se traduit par une meilleur interprétation des données dans le nouvel espace. Son principe consiste à construire successivement les composantes principales qui sont des combinaisons linéaires des observations, chaque composantes étant orthogonales à la suivante. Plus précisément, les colonnes de la matrice document-terme $A$ sont préalablement transformée en des variables centrées réduites\footnote{Centrer-réduire une variable $X$ d'espérance $\mu$ et d'écart-type $\sigma$ revien à obtenir une variable  $\hat{X}$ d'espérance nulle et variance à 1 en calculant pour chaque valeur $X_i$ de $X$, $\hat{X_i} = \frac{X_i-\mu}{\sigma}$.} résultant en une matrice $\hat{A}$. Ensuite, l'ACP décompose $\hat{A}$ la matrice $N\times m$ en un produit de trois matrices: $U$ la matrice $N \times N$  des vecteurs propres de $\hat{A}\hat{A}^T$, $S$ la matrice diagonale $m\times m$  des valeurs propres de la plus grande à la plus petite, et $V$ la matrice $m\times m$  des vecteurs propres de $\hat{A}^T\hat{A}$. Un nombre $q$ de composantes (vecteurs propres $U_q = U[1,m;1,q]$) peut être sélectionné pour la réduction de dimension suivant la variance totale qu'on souhaite conserver. Ainsi, tout vecteur $\vec{d}$ est réduit à dimension en le multipliant par $U_q$: $\vec{d}_q = U_q\vec{d}$.
Le nombre $q$ de composantes peut-être sélectionné par la règle de Kaiser qui juge que seules les valeurs propres supérieures ou égales à la moyenne des valeurs propres sont plus informatives que les variables initiales. %La réduction de dimension par l'ACP consiste à ... . Avantage et limite

\paragraph[LDA]{Allocation latente de Dirichlet}
L'allocation latente de Dirichlet (ALD) \citep{blei2003lda} est une technique qui extrait un nombre $q$ de thématiques qui transparaissent dans un corpus. Elle introduit les "variables latentes" comme pont pour modéliser les relations entre les textes et les termes. Chaque document est caractérisé comme une distribution de Dirichlet sur les variables latentes (thématiques), et chaque thématique est caractérisée par une autre distribution de Dirichlet sur tous les termes. La réduction de dimension est déduite de la première distribution en obtenant pour chaque document, un vecteur de taille $q$ représentant la distribution de probabilité des thématiques dans le document. Un des défi de la modélisation thématiques est la détermination d'une valeur optimale du nombre de thèmes $q$. Parmi plusieurs valeurs candidates, celle qui maximise la cohérence du modèle peut être choisie. La cohérence do modèle est la moyenne des  cohérences des thèmes. La cohérence de chaque thème peut être considérée comme étant la moyenne de la similarité entre les paires de termes de la description du thème \footnote{les premiers termes les plus fréquents du thème}. \citet{fang2016w2v_for_topiccoherence} démontrent que la cohérence est mieux estimée avec la similarité cosinus des plongements lexicaux des termes, qu'avec les métriques basées sur l'information mutuelle ponctuelle et sur l'analyse latente sémantique. % La réduction de dimension par l'ALD consiste à ... . Avantage et limite

\paragraph[LSA]{Analyse latente sémantique}
L'analyse latente sémantique (ALS) \citep{dumais1988LSI, deerwester1990indexingbyLSA} réalise une décomposition en valeurs singulières (SVD) de la matrice $A$ des scores de co-occurrence document-terme où chaque cellule contient le poids du terme dans le document correspondant (par exemple TF-IDF). Plus précisément, l'ALS applique la décomposition SVD directement sur $A$, contrairement à l'ACP qui centre-réduit cette dernière au préalable. L'ALS permet ainsi de réduire la grande dimension lexicale des documents à un espace de thématiques défini par le nombre de valeurs propres choisies. Comme pour l'ALD, le nombre $q$ de composantes (ou thématiques) peut être sélectionné comme étant celui qui  maximise la cohérence. 
% La réduction de dimension par l'ALS consiste à ... . Avantage et limite

\paragraph[NMF]{Factorisation de matrice non-négative} Un algorithme de factorisation de matrice non-négative (FMN) \citep{paatero1994nmf} factorise la matrice $X$ des scores non-négatifs de co-occurrence document-terme, en deux matrices $W$ et $H$ sans élément négatif, dont le produit est une approximation de $X$. Il s'agit en effet d'une méthode de modélisation de thématiques dans laquelle $W$ est décrite comme la matrice $N\times q$ terme-thème i.e. la matrice de la relation entre les termes et les thèmes découverts dans les documents de $X$, et $H$ est la matrice $q\times m$ thème-document i.e. les poids d'appartenance des documents aux thèmes. Le calcul de $W$ et $H$ consiste à optimiser (minimiser) la fonction objective: \[\frac{1}{2}\norm{X - WH}^2_F = \sum\limits_{i=1}^N\sum\limits_{i=1}^m (X_{ij} - (WH)_{ij})^2,\] où $\norm{\cdot}_F$ désigne la norme matricielle de Frobenius\footnote{$\norm{X}_F = \sqrt{\sum\limits_{i=1}^N\sum\limits_{i=1}^m \vert X_{ij}\vert^2}$}, et $X_{ij} = w(t_j, d_i)$, $d_i \in \mathcal{D}$.  %La réduction de dimension par la FMN consiste à ... . Avantage et limite
Comme pour l'ALD, le nombre $q$ de composantes (ou thématiques) peut être sélectionné comme étant celui qui  maximise la cohérence. 


\paragraph[w2v*TF-IDF]{Barycentre des termes}
A partir de vecteurs de mots apprise d'entraînement à l'aide de méthodes comme Word2Vec \citep{mikolov2013word2vec} ou GloVe \citep{pennington2014glove}, il est souvent proposé de considérer le document comme le barycentre des mots qu'il contient, et de le représenter comme la moyenne des vecteurs de termes pondérés par leur poids dans le document (par exemple TF-IDF) \citep{lemikolov2014word2vec, charlet2017simbow_tal,arora2017wordAvgSentEmbedd}. En effet, tout vecteur $\vec{d}$ est réduit en un vecteur $\vec{d}_q$ de $q$ dimensions, ce dernier étant le nombre de dimensions des vecteurs de termes:\[\vec{d}_q[k] = \frac{1}{\sum\limits_{i=1}^m \vec{d}[i]} \sum\limits_{i=1}^{m}\vec{d}[i]*\vec{t_i}[k].\]
%Pour de longs documents comme les décisions de justice, la moyenne des vecteurs court le risque de ne pas être pertinente. 

\subsection{Sélection du nombre optimal de clusters}
\label{sec:similarite:k-optimal}
%\textcolor{red}{faire un tableau des indices comme dans l'article, et comparer les combinaison indices-algo-distance}

 Au delà de l’algorithme à utiliser, le nombre $K$ approprié de clusters doit être déterminé mais pas prédéfini, puisqu'il est difficile de savoir à l'avance le nombre de groupes. Une méthode très connue est celle du \og coude \fg{}  (ou \og genou \fg{}) \citep{halkidi2001clustvalidation}, qui est basé sur le principe de base des algorithmes de partitionnement (e.g. K-moyennes) i.e. minimiser le critère d'inertie\footnote{la variance intra-cluster qui est la somme au carré des erreurs (distance d'un membre au centre).}:
\[J(K) = \sum\limits_{j=1}^K\sum\limits_{d_i \in C_j}\norm{\vec{d}_i-\overline{\vec{d}_j}}^2\]

$C_j$: ensemble des objets du cluster $j$

$\overline{\vec{d}_j}$: échantillons moyens du cluster $j$ (centre)

La méthode du coude consiste à essayer différentes valeurs consécutive de $K$ (de $K_{min}$ à $K_{max}$) puis de choisir celle qui correspond au coude de la courbe du critère d'inertie $J(K)$. Le choix de ce coude est visuel et peut être ambigu (plusieurs valeurs de $K$ sur le coude par exemple). 

La méthode de la silhouette moyenne \citep{rousseeuw1987silhouetteclusternumber} est une alternative qui consiste à choisir comme valeur optimale de $K$, celle qui maximise le critère de la largeur moyenne de la silhouette: $\overline{s_K}(C) = \frac{1}{K}\sum\limits_{i=1}^N s(d)$. La largeur $s(d)$ de la silhouette est un indice qui compare la ressemblance d'un document $d$ aux autres membres de son cluster $C_t$ par rapport à ceux d'autres clusters $C_l, l \neq t$:
\[s(d) = \frac{b(d) - a(d)}{\max\lbrace a(d),b(d)\rbrace}\]

où $a(d) = \frac{1}{\vert C_t \vert} \sum\limits_{d' \in C_t} Dis(d, d')$, et $b(d) = \min\limits_{l \neq t} \frac{1}{\vert C_l \vert} \sum\limits_{d' \in C_t} Dis(d, d')$, pour $d \in C_l$.

$K$ est optimal lorsque la largeur moyenne $\overline{s_K}(C)$ est maximale. Les valeurs de ce dernier varient entre -1 (pire valeur) et +1 (meilleure valeur). Les valeurs proches de zéro indiquent que les clusters se chevauchent en $x$, et il est difficile de savoir à quel cluster $x$ doit être affecté. Une valeur négative indique que $x$ a été affecté à cluster inapproprié.

%Plus récemment, \citet{tibshirani2001gap_statistic} proposent la méthode de "statistique d'écart" (\textit{gap statistic}) dont le principe est de sélectionner le $K$ dont l'erreur sur le clustering de $\mathcal{D}$ est le plus éloignée possible de l'erreur moyenne obtenu sur les clusterings de $M$ ensembles $\mathcal{V}_q, 1\leq q \leq M$ de $N$ vecteurs aléatoirement générés. L'erreur sur le clustering de l'ensemble $\mathcal{A}$ est  $W_K(\mathcal{A})=\sum\limits^K_{r=1}\frac{1}{2n_r}\sum\limits_{i,i' \in C_r} (Dis(i,i'))^2$, $C_r$ étant le $r^\text{ième}$ cluster de taille $n_r$ de $\mathcal{A}$. Le nombre optimal de clusters est la valeur de $K$ qui maximise $Gap_M(K) = \frac{1}{M}\sum\limits_{q=1}^M \log{W_K(\mathcal{V}_q)} - \log{W_K(\mathcal{A})}$ parmi un ensemble de candidats données.

%Etant donné le grand nombre de méthodes existantes \citep{liu2010interclustvalidation, Amorim2015recoveringnumclust}, la majorité des votes peut être appliquée pour choisir le bon $K$ \footnote{\url{https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/}}.

%\subsection{Initialisation des centroïdes}

%\subsection{Définir une représentation appropriée pour les textes}
%\url{https://arxiv.org/pdf/1509.01626.pdf}

%\url{http://ad-publications.informatik.uni-freiburg.de/theses/Bachelor_Jon_Ezeiza_2017_presentation.pdf}


%\subsection{Labeliser les clusters}

\subsection{Validation de la catégorisation}
La validation d'une catégorisation peut être supervisée ou non selon que des documents labellisés dans les groupes attendus (données externes) sont employées.

\subsubsection{Métriques supervisées ou mesures externes}
\label{sec:similarite:biblio:supeval}
Les métriques couramment employées mesurent la ressemblance entre deux catégorisations $X = \lbrace X_1, X_2,..., X_r \rbrace$ et $Y = \lbrace Y_1,Y_2,..., Y_s \rbrace$:
\begin{itemize}
	\item l'indice ajusté par chance de Rand (\textit{ajusted Rand index} - ARI) \citep{hubert1985adjustedrandidx} corrigent l'indice de Rand (RI) \citep{rand1971randidx} pour obtenir une valeur très proche de 0.0 pour les catégorisations aléatoires et exactement 1.0 lorsque les clusters sont identiques aux classes attendues. En effet, l'indice de Rand prend ses valeurs en pratique dans l'intervalle $[0.5;1]$, et a par conséquent une valeur de base très élevée. ARI est calculé à l'aide du tableau de contingence résumant les chevauchements que partagent $X$ et $Y$ (Tableau \ref{tab:similarite:tab-contingence}) par la formule: %\[ARI(Y,C) = \frac{RI(Y,C) - E_{perm}\left[RI(Y,C)\right]}{1.0 - E_{perm}\left[RI(Y,C)\right]} \]
	\[ARI(X,Y) = \frac{\sum\limits_{i,j}\binom{n_{ij}}{2} - \frac{\sum\limits_{i}\binom{a_{i}}{2}\sum\limits_{j}\binom{b_{j}}{2}}{\binom{N}{2}}}{\frac{1}{2}\big[\sum\limits_{i}\binom{a_{i}}{2}+\sum\limits_{j}\binom{b_{j}}{2}\big] - \frac{\sum\limits_{i}\binom{a_{i}}{2}\sum\limits_{j}\binom{b_{j}}{2}}{\binom{N}{2}}}, \text{ avec } \binom{n}{2} = \frac{n(n-1)}{2}.\]
	
	\begin{table}[!htb]
		\centering \small
		\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			& $Y_1$    & $Y_2$    & $\cdots$ & $Y_s$    & $\sum$   \\ \hline
			$X_1$    & $n_{11}$ & $n_{11}$ & $\cdots$ & $n_{11}$ & $a_1$    \\ \hline
			$X_2$    & $n_{21}$ & $n_{21}$ & $\cdots$ & $n_{21}$ & $a_2$    \\ \hline
			$\cdots$ & $\cdots$ & $\cdots$ & $\ddots$ & $\cdots$ & $\cdots$ \\ \hline
			$X_r$    & $n_{r1}$ & $n_{r1}$ & $\cdots$ & $n_{r1}$ & $a_r$    \\ \hline
			$\sum$   & $b_1$    & $b_2$    & $\cdots$ & $b_s$    &          \\ \hline
		\end{tabular}
		\caption{Tableau de contingence des chevauchement entre les catégorisations $X = \lbrace X_1, X_2, ..., X_r \rbrace$ et $Y = \lbrace Y_1, Y_2, ..., Y_s \rbrace$, $n_{i,j} = \vert X_i \cap Y_j\vert$} \label{tab:similarite:tab-contingence}
	\end{table}
ARI a des valeurs dans $[-1;1]$. Une valeur négative indique que le catégorisation obtenu s'accorde moins bien avec l'attendu qu'une catégorisation aléatoire. 
	% \item la précision du clustering (ACC) \citep{}.   \textcolor{red}{pas utilisé car nécessite de trouver le meilleur mapping entre les classes et les clusters}
	\item L'information mutuelle normalisée (NMI) \citep{kvalseth1987entropy_NMI,strehl2000nmi, vinh2010clusteringComparison} normalise l'information mutuelle entre les catégorisations $X$ et $Y$ par une agrégation de leur entropie respective. Par exemple, l'incertitude symétrique \citep{kvalseth1987entropy_NMI} utilise la moyenne comme agrégateur:  $NMI(X,Y) = \frac{2 \cdot I(X,Y)}{H(Y) + H(Y)} $, 
	avec $I(X,Y) = H(X) - H(X \vert Y) = \sum\limits_{i=1}^{r}\sum\limits_{j=1}^{s} \frac{n_{ij}}{N} \log_2\frac{{n_{ij}}/{N}}{{a_ib_j}/{N}}$ et $H(X) = \sum\limits_{X_i \in X}\left(- p(X_i)\log_2 p(X_i)\right)$,  $p(X_i) = \frac{a_i}{N}$ et $p(Y_j) = \frac{b_j}{N}$; $n_{ij}, a_i$ et $b_j$ provenant du tableau de contingence (Tableau \ref{tab:similarite:tab-contingence}). La meilleure catégorisation est celui qui a la plus grande valeur.
	
	\item Au regard de leur formulation, les métriques ARI et NMI vérifie la différence des proportions entre les clusters de deux catégorisations indépendamment des affectations des documents. D'autres méthodes appelées mesures de comptage de pair (\textit{pair counting measures}) mesurent la capacité du modèle à mettre deux documents similaires (de labels identiques dans les données annotées) dans le même groupe, et des documents dis-similaires (de labels différents dans les données annotées) dans des clusters différents. Par exemple, des mesures de précision, rappel, et F1-mesure sont définies par les formules suivantes \citep{manning2009irbook-flatclustering}:
	\[P = \frac{{TP}}{{TP} + {FP}}, R = \frac{{TP}}{{TP} + {FN}}, F1 = \frac{2 \times P \times R}{P + R}.\]
	Ces scores prennent leurs valeurs dans $[0;1]$. Les métriques de bases vrais/faux positifs/négatifs qui servent à les calculer, sont définies comme suit:
	\begin{itemize}
		\item un vrai positif (TP) survient si le modèle place deux documents similaires dans le même cluster (groupe généré par le modèle);
		\item un faux négatif (FN) survient si deux documents similaires sont dans des clusters différents;
		\item un vrai négatif (TN) est une décision qui place deux documents dissemblables dans deux clusters différents;
		\item un faux positif (FP) survient si deux documents dissemblables sont dans le même cluster.
	\end{itemize}
		
\end{itemize}
%Ces métriques doivent être utilisées ensemble  pour compenser les limites de chacune d'elles \citep{yang2017kmeansfriendlyspaces}.



\subsubsection{Métriques non-supervisées ou indices internes}
\label{sec:similarite:biblio:unsupeval}

 La cohésion et la séparation des clusters sont les principaux indices internes. La cohésion mesure le degré de proximité entre objets d'un cluster à partir du carré de la somme des erreurs\footnote{Erreur: distance entre un point et le centre du cluster dont il est membre.} dans les clusters: $WCSS(C) = \sum\limits_{j=1}^K\sum\limits_{x \in C_j} (Dis(x, m_j))^2$, où $C = \lbrace C_1, C_1, \cdots, C_K \rbrace$ est l'ensemble des clusters de la catégorisation, $m_j$ le centre de $C_j$, et $Dis(x,m_j)$ la distance (généralement euclidienne) entre un point $x$ et $m_j$. En général, une valeur faible de la cohésion indique que les clusters sont plus compactes, et donc de meilleur qualité. Tandis qu'une valeur élevée révèle une grande variabilité entre les objets à l'intérieur les clusters. La séparation quant à elle mesure l'éloignement de chaque cluster des autres à partir du carré de la somme des distances entre clusters: $BCSS(C) = \sum\limits_{j = 1}^{K} \vert C_j \vert (m - m_j)$, 
  $m$ étant le centre l'ensemble des objets (la moyenne des vecteurs de tous les documents, où le document qui se trouve à une distance moyenne minimale de tous les autres). Une grande valeur de séparation indique que les clusters sont isolés les uns des autres, et par conséquent elle doit être maximisée pendant la catégorisation. Le coefficient de silhouette de \citet{rousseeuw1987silhouetteclusternumber} (cf. \ref{sec:similarite:k-optimal})  combine les idées de cohésion et séparation mais pour chaque objet.

\section{Apprentissage d'une distance basée sur la transformation de document}
Nous définissons la métrique suivante qui est fonction des transformations permettant de passer d'un document $d$ à un autre $d'$ :
\begin{equation}
\begin{array}{cccccc}
Dis_\mathcal{M} & : & \mathcal{D} \times \mathcal{D} & \to & \mathbb{R} & \\
& & d, d' & \mapsto & Dis_{\mathcal{M}}(d, d') & = f(\mathcal{M}_{d, d'}). \\
\end{array} \label{eq:similarite:distance-modif}
\end{equation}

$\mathcal{D}$ est le corpus. $\mathcal{M}_{d, d'}$ est l'ensemble des modifications de $d$ permettant pour obtenir $d'$ i.e. les paires de mots différents $(d_{k}, d'_{k})$ telles que le mot $d_{k}$ à la position $k$ dans $d$ a été remplacé par $d_{k}$ à la position $k$ dans $d'$. $f$ est une fonction qui croît avec le nombre de modifications. Après une légère modification, le sens d'un texte reste assez similaire à celui de l'original. Tandis qu'après un grand nombre de modifications, le sens du texte sera très différent de l'original. 

Pour des textes de même taille, toute formulation mathématique permettrait de calculer $Dis_\mathcal{M}$ car il est facile de faire correspondre les mots par leur position. Par exemple, cette distance peut donc se formuler comme étant la proportion de mots modifiés:
\begin{equation}
{Dis_\mathcal{M}}(d,d') = {f}(\mathcal{M}_{(d,d')}) = \frac{\vert \mathcal{M}_{(d,d')} \vert}{\vert d_i \vert } \label{eq:similarite:taux-modif}
%  $\vert d_i \vert$ étant le nombre de mots de $d_i$.
\end{equation}
 Par contre, pour des textes de tailles différentes, il est impossible de savoir les positions où des mots ont été supprimés ou ajoutés, et par conséquent, il devient impossible d'estimer leur distance par une formule. La distance étant une valeur continue, en entraînant un modèle de régression sur un ensemble de paires de documents pour lesquelles on connaît la distance, il est possible de la prédire pour des paires de textes de tailles quelconques. Nous proposons de générer une base synthétique de paires de documents dont l'un est un document du corpus original mais l'autre est le résultat de substitutions et suppression de mots du premier. En contrôlant ces modifications, il est facile de calculer une valeur de $Dis_\mathcal{M}$ pour chaque paire générée de documents, même s'ils sont de tailles différentes (en considérant la suppression d'un mot comme son remplacement par le \og mot vide \fg{} considéré comme faisant partie de l'ensemble $W$ de mots).

\subsection{Génération d'une base d'apprentissage}
La génération de la base synthétique nécessite de définir une formulation de la fonction $f(\mathcal{M}_{d, d'})$ pour les documents de taille égale, comme par exemple celle de l'Equation \ref{eq:similarite:taux-modif}. Cette formulation ne considérant pas la similarité entre les mots substituants et les remplacés, nous proposons de pondérer chaque modification par la distance entre les mots substitués (le vecteur du  \og mot vide \fg{} étant nul):
\begin{equation}
{Dis_\mathcal{M}}(d,d') = {f}(\mathcal{M}_{(d,d')}) = \frac{\sum\limits_{(d_k, d'_k) \in \mathcal{M}_{(d,d')}} Dis_{cos}(\vec{d_k}, \vec{d_{ik}})}{\vert d_i \vert} \label{eq:similarite:somme-dist-mots}
\end{equation}
$d_i$ est un document du corpus original $\mathcal{D}$, et $d'$ est le document synthétique obtenu par modifications contrôlées de $d_i$. $\vec{d_k}$ désigne le plongement lexical du mot $d_k$. Pour garantir la symétrie et la réflexivité de la métrique, nous imposons respectivement ${Dis_\mathcal{M}}(d,d') = {Dis_\mathcal{M}}(d', d_i)$ et ${Dis_\mathcal{M}}(d_i, d_i) = {Dis_\mathcal{M}}(d', d') = 0, \forall d_i \in \mathcal{D}$ sur le jeu d'entraînement généré. L'algorithme de génération de document synthétique utilise une valeur seuil de probabilité $0<p<1$ contrôlant le taux de modifications à effectuer sur le document original (Algorithme \ref{algo:similarite:modifierdoc}). 

\begin{algorithm}[!htb] % Version française avec : https://pierre.chachatelier.fr/latex/index.php
	\footnotesize
 \KwData{document $d_i \in \mathcal{D}$, valeur seuil $p$, ensemble $W$ des mots}
 \KwResult{$d'$, $\mathcal{M}_{(d,d')}$}
 ${d'} = [] $\; 
 $\mathcal{M}_{(d,d')} = \emptyset$\;
 \For{ $k \in [1\twodots \setsize{d_i}]$ }{
 	$v = random(0,1)$\;
    \eIf{v < p}{
       $d'_k = modifier\_mot(d_k, W)$; // mot aléatoire de $W$ différent de $d_k$\;
       $\mathcal{M}_{d_k,d'_k} = \mathcal{M}_{(d,d')} \cup \lbrace (d_k,d'_k) \rbrace$\;
     }{
     $d'_k = d_k$\;
     }     
 }
 \Return $d', \mathcal{M}_{(d,d')}$\;
 \caption{\textit{modifier\_document($d_i, p, W$)}} \label{algo:similarite:modifierdoc}
\end{algorithm}

Pour une même valeur ou des valeurs différentes de $p$, plusieurs documents sont ainsi générés pour chaque $d^i \in \mathcal{D}$ pour former un ensemble de données d'entraînement $B_\mathcal{M} = \lbrace (d^i, {d^i}', {Dis}(d^i, {d^i}'))\rbrace_{1 \leq i \leq \setsize{B_\mathcal{M}}}$.

\subsection{Entraînement de la métrique}

Sur $B_\mathcal{M}$, on entraîne un modèle régressif $m$ pour prédire la distance entre deux documents quelconques en fonction de leur représentation vectorielle. Ce modèle de régression $Reg_\mathcal{M}$ peut être utilisé comme distance dans un algorithme de catégorisation comme celui des K-moyennes. Cependant, les modèles de régression ne supportent généralement qu'un seul vecteur en entrée, et pas deux comme en dispose la base $B$. Les vecteurs $d_i$ et $d'$ doivent donc être agrégés en un seul. L'agrégation qui fonctionne le mieux est la soustraction avec laquelle les documents similaires auront une agrégation avec un grand nombre de composantes tendant vers 0. La fonction d'estimation automatique de la distance sémantique entre $x$ et $y$ s'écrit: $Dis_\mathcal{M}(d, d') = Reg_\mathcal{M}(\vec{d_{i}} - \vec{d_{j}})$. 

\subsection{Utilisation pour la catégorisation non supervisée des documents}
 Nous proposons dans un premier temps de sélectionner la représentation vectorielle pour laquelle la distance sépare au mieux les groupes manuellement annotées et rapproche au mieux les éléments à l'intérieur de ces groupes. La représentation\footnote{Combinaison modèle vectoriel et méthode de réduction} optimale maximise la largeur moyenne de la silhouette de la catégorisation manuelle. L'idée étant d'avoir une représentation qui optimise à la fois les métriques supervisées et non supervisées de validation. La représentation vectoriel ainsi déterminée est utilisée ensuite pour la catégorisation des corpus non annotés. % Ensuite, La distance apprise peut être utilisée dans des algorithmes comme les k-moyennes sur des documents représentés sous la forme optimale sélectionnée.

%\textcolor{red}{Issues:}
%\begin{itemize}
%\item les docs sont généralement de tailles différentes, ne faudrait il pas intégrer une perturbation ajout de mots? \textcolor{blue}{la suppression peut être considérée comme le remplacement d'un mot par le mot vide, qui doit être ajouté aux word2vec}
%\item il faudrait intégrer la composante taille du document: \textcolor{blue}{agréger sur le nombre minimal de phrases des paires de documents}
%\item comment assurer les propriétés d'une fonction similarité? par exemple si aucune perturbation n'est opérée, alors la similarité est maximale et si tous les mots sont modifiés alors la similarité est minimale: \textcolor{blue}{agrégation par soustraction des vecteurs du couple de docs. plus deux doc seront similaire, plus le vecteur de leur paire tendra vers le nul}
%\item Ne faudrait il pas prendre en compte un poids pour les mots, car peut-être la modification de certains mots ne devrait pas avoir le même impact sur la similarité ou le taux de perturbation que celle d'autres mots:  \textcolor{blue}{lissage par la somme des distance des vecteurs de mots substitués Eq. \ref{equation:similarite:somme-dist-mots}}
%\item ne faudrait il pas intégré une métrique proche de la tâche: la ressemblance n'est pas forcément globale à tous le corps du document mais plus à certaines régions; donc un document auquel on rajoute quelques phrases ne devrait pas voir  son sens trop changer:  \textcolor{blue}{peut-être agréger les distances minimales entre les paires de phrases}
%\end{itemize}

\section{Expérimentations et résultats}
\label{sec:similarite:experimentations}
Cette section discute la validité, l'adéquation, et l'efficacité de la métrique apprise en comparaison avec d'autres métriques d'estimation de la similarité sémantique entre documents. La validité de la métrique est établie si cette dernière respecte les propriétés d'une distance. L'adéquation de la métrique avec le problème à résoudre mesure la capacité de la métrique à estimer une distance très faible entre documents du même label (catégorisation manuelle), en même temps qu'une similarité quasi nulle entre documents de labels différents, indépendamment de l'algorithme de catégorisation appliqué. Enfin, l'efficacité de la métrique est liée à la qualité de catégorisation  résultant de l'application d'un algorithme de catégorisation  combiné avec la métrique apprise.


\subsection{Données}
Pour l'évaluation supervisée, nous disposons d'une base annotée sur la catégorie de demande "dommage-intérêts / action en responsabilité civile professionnelle contre les avocats" (\textit{arcpa}) qui concerne les contentieux impliquant des avocats.  L'expert annotateur a annoté 81 documents\footnote{Sur 85 documents disponibles de catégorie \textit{arcpa}} avec 6 documents appartenant chacun à 2 circonstances factuelles (chevauchements) parmi les 4 identifiées :
\begin{itemize}
\item cas $a$ (46 documents): il s'agit d'un avocat qui est négligent et envoie son assignation de manière tardive; %(champ sémantique: retard/délai/prescription)
\item cas $b$ (20 documents): il s'agit d'un avocat qui n'a pas donné un conseil opportun, qui n'a pas soulevé le bon argument;
\item cas $c$ (18 documents): un avocat qui n'a pas rédigé un acte valide ou réussi à obtenir un avantage fiscal; % (champ sémantique: rédacteur d'actes)
\item cas $d$ (3 documents): il s'agit d'un avocat attaqué par son adversaire et non par son propre client.
\end{itemize}


%\begin{figure}[!htb]
%	\centering \includegraphics[scale=0.5]{arcpa-data-distrib.png}
%	\caption{Répartition des documents annotés par circonstances factuelles (\textit{arcpa}).}\label{fig:similarite:arcpa-data-distrib}
%\end{figure}

Les terminologies des différents cas se distinguent bien (Tableau \ref{tab:similarite:terminologie-resp_avocat}), et par conséquent, une représentation des documents qui mettrait en évidence ces différents termes permettrait de retrouver les circonstances factuelles associées. 
\begin{table}[ht]
	\centering \scriptsize
	\begin{tabular}{|l|p{.85\textwidth}|}
		\hline
		\textbf{Corpus} & \textbf{Terminologie} \\ \hline
		\textit{arcpa} & perte de chance, 
		responsabilité civile professionnelle, 
		civile professionnelle, 
		chance d' obtenir, 
		faute de l' avocat, 
%		chance, 
%		contradictoirement après débats en audience, 
%		nécessairement rejet, 
%		avocat est tenu, 
%		obtenir gain, 
%		obtenir gain de cause, 
%		responsabilité professionnelle, 
%		audience publique et après, 
%		publique et après, 
%		chances, 
%		aucune chance, 
%		perte de chance d' obtenir, 
%		délibéré statuant publiquement, 
%		perdu une chance, 
%		fait perdre une chance, 		
		\\ \hline
		\textit{cas a} & obtenir gain
		obtenir gain de cause
		perte de chance
		perdu une chance
		chances
%		nécessairement rejet
%		chance
%		confié la défense
%		conseil' attendu
%		chances d' aboutir
%		appel que ces condamnations
%		nécessairement rejet de la demande
%		appel que ces condamnations emportent
%		emportent nécessairement
%		condamnations emportent
%		condamnations emportent nécessairement rejet
%		condamnations emportent nécessairement
%		emportent nécessairement rejet
%		projet d' assignation
%		toute chance		
		\\ \hline
		\textit{cas b} & avocat au barreau de dijon
		412 du code de procédure
		barreau de dijon
		412 du code
		devoir de compétence
%		régularité et d' autre part
%		obliger qu' en l' espèce
%		emportant pouvoir
%		régime de la responsabilité professionnelle
%		régularité et d' autre
%		représentation en justice lui imposant
%		relève de la responsabilité contractuelle
%		imposant d' accomplir
%		dispositions combinées des articles 411
%		avocat est investi
%		avocats relève
%		responsabilité professionnelle des avocats
%		devoir de conseiller la partie
%		tous les actes nécessités
%		appel et le jugement entrepris		 
		\\ \hline
		\textit{cas c} &avocat rédacteur d' un acte
		qualité de rédacteur d' acte
		rédacteur d' acte
		rédacteur d' un acte
		avocat rédacteur
%		7.2 du règlement intérieur
%		7.2 du règlement
%		article 7.2 du règlement
%		article 7.2 du règlement intérieur
%		deux parties à la convention
%		validité et la pleine
%		portée et les incidences
%		initiative de conseiller les deux
%		initiative de conseiller
%		prévisions des parties
%		prendre l' initiative de conseiller
%		présence et de prendre
%		conseiller les deux parties
%		conseiller les deux
%		convention sur la portée
		\\ \hline
		\textit{cas d} & instances introduites
		chaque mois sous
		référé du 5 mai
		réunies car
		référé renvoyons
%		réception du 20 avril 2009
%		référés compte
%		référé du 22 novembre
%		référé du 27 novembre
%		obtenir une décision différente
%		référés compte tenu
%		l. doivent être condamnés
%		appartenait aux appelants
%		appelant il n' en demeure
%		appel relativement à la juridiction
%		appel que les dépens
%		apparaît constituer
%		valoir tous
%		66-879 du 29
%		66-879 du 29 novembre 1966				 
		\\ \hline
	\end{tabular}

	\textit{5 premiers termes de 1 à 5 mots sélectionnés à l'aide du coefficient de corrélation $ngl$ (cf. \ref{sec:quanta:poids-globaux-superv})}
	\caption{Terminologies  de la catégorie \textit{arcpa}, et de ses circonstances factuelles annotées.}\label{tab:similarite:terminologie-resp_avocat}
\end{table}

Pour l'évaluation non supervisée, les corpus des catégories de demande des chapitres \ref{chap:quanta} et \ref{chap:sensresultat} sont employés en plus pour l'évaluation non-supervisée. Nous les appelons respectivement $\mathcal{D}_{acpa}$, $\mathcal{D}_{concdel}$, $\mathcal{D}_{danais}$,$\mathcal{D}_{dcppc}$, $\mathcal{D}_{doris}$, $\mathcal{D}_{styx}$.

\subsection{Protocole et outils logiciels}
Pour analyser la validité et l'adéquation de la métrique apprise, nous l'entraînons sur la base générée puis nous l'évaluons sur le corpus annoté $\mathcal{D}_{arcpa}$ restreint aux 74 documents n'appartenant qu'à l'un des cas $L = \lbrace a, b, c \rbrace$. Quant à l'efficacité de la métrique apprise, nous l'entraînons sur toutes les données annotées générées. Les documents sont pré-traités avant leur représentation sous forme vectorielle. Ce pré-traitement consiste à sectionner les documents (chapitre \ref{chap:structuration}), à n'utiliser que la section Motifs, à mettre en minuscule et lemmatiser ce contenu, puis à éliminer la ponctuation et des mots inutiles (\textit{stop words})  car ils sont généralement indépendants de toute catégorie. Après pré-traitement, les documents ont une taille allant de 208 à 3812 mots dont une moyenne de 1381 mots. Pour générer les données  d'entrainement de $Dis_M$, le vocabulaire $W$ utilisé est restreint au mots du corpus original $D$ sur lequel il faut appliquer les catégorisations. La représentation vectorielle emploie les modèles de types TF-IDF (poids local $\times$ poids global) avec des n-grammes de 1 à 3 mots. Chaque poids global $g(t)$ d'un terme $t$ est multiplié par le logarithme du nombre de mots de $t$ donnant ainsi une nouvelle formulation $\hat{g}(t) = \log_2{\vert t \vert} \times g(t)$ qui, par conséquent, augmente le poids des longs termes qui sont rares mais parfois plus discriminants. Les poids globaux sont appris sur la discrimination entre deux corpus $\mathcal{D}_{C}$ (85 documents) et $\mathcal{D}_{\overline{C}}$ (427) i.e. la terminologie d'une catégorie de demande $C$. 427 documents forment $\mathcal{D}_{\overline{arcpa}}$.

 La librairie Python Scikit-Learn \citep{Pedregosa2011scikit-learn} a été utilisée pour les implémentations des algorithmes de réduction de dimension. La distance WMD est celle implémentée dans la librairie Gensim de \citet{rehurek2010gensim}. Le modèle de vecteurs de mots est un modèle GLoVE de \citet{pennington2014glove} entraîné sur un corpus de +800k décisions de justice lémmatisées, à l'aide de vecteurs de dimension 300 obtenu sur une taille de fenêtre de contexte égale à 15 mots. Le vecteur $\vec{t}$ d'un terme $t$ de $n$ mots $(w_1, w_2, \cdots, w_n)$ est obtenu en concaténant leur vecteur respectif de la droite vers la gauche: $\vec{t}=[\vec{w_1}\vec{w_2}\cdots \vec{w_n}]$.

\subsection{Validité de la distance apprise}
La base d'entraînement $B$ comprend 935 documents dont 10 documents synthétiques générés pour chacun des 85 documents. Sur un modèle TF-IDF, la régression linéaire approxime bien la distance proposée $Dis_M$ car elle a un faible taux d'erreur (Figure \ref{fig:similarite:eval-regression}).

\begin{figure}[!htb]
 	\centering \includegraphics[width=0.5\textwidth]{eval-lr.png} \hfil
	%\includegraphics[width=0.49\textwidth]{eval-mlp.png}
	
	\caption{Différence entre valeurs prédites et  attendues par la distance apprise}\label{fig:similarite:eval-regression}
\end{figure}

Nous vérifions ici que la métrique respecte les propriétés des distances, et aussi si elle reste normale après l'entraînement. D'après la matrice des distances entre toutes les paires de document de la base annotées $\mathcal{D_{\text{arcpa}}}$ (Figure \ref{fig:similarite:distance_matrix}),  la "non-négativité", l'identité discernable, et la symétrie sont respectée car toutes les valeurs sont non-négative, seule la diagonale est nulle, et la matrice est symétrique. De plus, toutes les distances sont comprises entre 0 et 1, et par conséquent la métrique est normée (donc la similarité est déduite par $Sim_\mathcal{M} = 1 - Dis_\mathcal{M}$).

\begin{figure}[!htb]
	\centering \includegraphics[scale=0.4]{distance_matrix.png}
	\caption{Matrice des distances de la métrique apprise sur $\mathcal{D_{\text{arcpa}}}$}\label{fig:similarite:distance_matrix}
\end{figure}

L'inégalité triangulaire est vérifiée car $Dis_\mathcal{M}(d,d'') - (Dis_\mathcal{M}(d,d') + Dis_\mathcal{M}(d',d'')) \leq 0, \forall (
d,d,d') \in \mathcal{D_{\text{arcpa}}} \times \mathcal{D_{\text{arcpa}}} \times \mathcal{D_{\text{arcpa}}}$ (Figure \ref{fig:similarite:matrice_inegalite_triangulaire}).

\begin{figure}[!htb]
	\centering \includegraphics[width=0.8\textwidth]{inegalite_triangulaire.png}
	
	\scriptsize{$Dis_\mathcal{M}(d,d'') - (Dis_\mathcal{M}(d,d') + Dis_\mathcal{M}(d',d'')), \forall (d,d',d'') \in \mathcal{D_{\text{arcpa}}} \times \mathcal{D_{\text{arcpa}}} \times \mathcal{D_{\text{arcpa}}}$, avec les $d$ indicés en lignes et les paires $(d',d'')$ en colonnes.}
	\caption{Matrice de vérification de l'inégalité triangulaire}\label{fig:similarite:matrice_inegalite_triangulaire}
\end{figure}
 

\subsection{Sélection de la représentation optimale des textes}
\label{sec:similarite:select-repr-optimal}
En considérant la catégorisation manuelle de $\mathcal{D}_{arcpa}$, différentes représentation vectorielles peuvent être comparées, à l'aide de la silhouette, sur leur habilité à séparer les clusters manuels de documents dans l'espace. Nous comparons ici les combinaisons de différents poids locaux (Tableau \ref{tab:quanta:metriq_locales}), poids globaux (cf. \ref{sec:quanta:extract-terminologie-domaine}) et méthodes de réduction de dimensions (cf. \ref{sec:similarite:reduction-dimension}). 
Le Tableau \ref{tab:similarite:silhouette-vecteur-manuel} présente la représentation de largeur moyenne optimale de silhouette sur les données annotées pour chaque distance. 

\begin{table}[!htb]
	\scriptsize \centering
	\begin{tabular}[pos]{|l|c|l|}
		\hline
		\textbf{Distance}&\textbf{Base$^a$}&\textbf{Silhouette optimale   (pondération, réduction, dim.)} \\ \hline
		$Dis_{jaccard}$ & 0.001 & 0.212 (TP-NGL, FNM, 4) \\ \hline
		$Dis_{cos}$ & 0.002 & 0.202 (TP-NGL, FNM, 4) \\ \hline
		$Dis_{M}$ & -0.049 & 0.195 (TP-NGL, FNM, 4) \\ \hline
		$Dis_{braycurtis}$ & 0.002& 0.182 (TP-NGL, FNM, 4) \\ \hline
		$Dis_{euclidienne}$ & 0.001& 0.168  (TP-NGL, FNM, 4) \\ \hline
		$Dis_{manhattan}$ & -0.019& 0.17   (TP-NGL, FNM, 4) \\ \hline
		$Dis_{pearson}$ & 0.014 & 0.057 (TP-CHI2, aucun, 19763) \\ \hline
		$Dis_{wmd}$ & -0.096 &  - \\ \hline
	\end{tabular}

	$^a$ occurrence de mots pour $Dis_{wmd}$, et TF-IDF pour les autres
    \caption{Meilleures représentations sur la catégorisation manuelle.} \label{tab:similarite:silhouette-vecteur-manuel}
\end{table}

 
Le modèle vectoriel TP-NGL réduit à 4 dimensions par la factorisation de matrice non négative (FNM) est préférée par la majorité des distances. Nous remarquons que la pondération globale supervisée (NGL et CHI2) met en évidence non seulement la terminologie de la catégorie de demande mais aussi celle des circonstances factuelles associées. La FNM marche mieux en moyenne pour toutes les classes avec des silhouettes maximales comprises entre 0.052 et 0.212, suivie de l'ALS (0.048 -- 0.119) et de l'ACP (0.029 -- 0.079). Les performances maximales observées par l'ALD (-0.032 -- -0.001) sont moins bonnes la représentation sans réduction (0.001 -- 0.008). %La réduction par la méthode du barycentre des termes, ???
Les représentations sélectionnées sont utilisées dans les sections suivantes.
\subsection{Catégorisation du corpus annotées manuellement}
\subsubsection{Nombre de clusters prédéfini}
 Le Tableau \ref{tab:similarite:validation-supervisee-k3} présente les mesures ARI, NMI et F1-mesure de la catégorisation par k-means et k-medoids sur $\mathcal{D}_{arcpa}$, avec $K=3$. 

\begin{table}[!htb]
	\centering \scriptsize
	\begin{tabular}[pos]{|l|l|c|c|c|c|c|c|}
		\hline
		\textbf{Distance}& \textbf{Algorithme}& \textbf{Silhouette}& \textbf{ARI} & \textbf{NMI} & \textbf{R} & \textbf{P} & \textbf{F1} \\ \hline
		 dis\_m          & kmeans    & 0.403      & 0.411 & 0.427 & 0.574  & 0.648     & \textbf{0.607} \\ \hline
		 dis\_m          & kmedoids  & 0.398      & 0.321 & 0.340 & 0.483  & 0.591     & 0.532 \\ \hline
		 dis\_braycurtis & kmeans    & 0.370      & 0.364 & 0.382 & 0.545  & 0.603     & 0.570 \\ \hline
		 dis\_braycurtis & kmedoids  & 0.358      & 0.272 & 0.292 & 0.444  & 0.540     & 0.487 \\ \hline
		 dis\_cosine     & kmeans    & 0.422     & 0.389 & 0.406 & 0.556  & 0.616     & 0.583 \\ \hline
		 dis\_cosine     & kmedoids  & \textbf{0.448}      & 0.437 & \textbf{0.455} & 0.656  & 0.598     & \textbf{0.626} \\ \hline
		 dis\_euclidean  & kmeans    & 0.372      & \textbf{0.417} & \textbf{0.434} & 0.591  & 0.603     & 0.592 \\ \hline
		 dis\_euclidean  & kmedoids  & 0.369      & 0.392 & 0.409 & 0.566  & 0.672     & 0.615 \\ \hline
		 dis\_jaccard    & kmeans    & \textbf{0.442}      & 0.371 & 0.389 & 0.554  & 0.600     & 0.574 \\ \hline
		 dis\_jaccard    & kmedoids  & 0.431      & \textbf{0.440} & \textbf{0.455} & 0.529  & 0.645     & 0.581 \\ \hline
		 dis\_manhattan  & kmeans    & 0.390      & 0.376 & 0.394 & 0.567  & 0.582     & 0.571 \\ \hline
		 dis\_manhattan  & kmedoids  & -0.059     & 0.097 & 0.127 & 0.479  & 0.422     & 0.448 \\ \hline
		 dis\_pearson    & kmeans    & 0.434      & 0.088 & 0.117 & 0.585  & 0.487     & 0.530 \\ \hline
		 dis\_pearson    & kmedoids  & -0.019     & 0.111 & 0.136 & 0.421  & 0.476     & 0.447 \\ \hline
		 dis\_wmd    & kmedoids  & 0.105 & -0.004&	0.024&	0.333&	0.401&	0.364 \\ \hline
	\end{tabular}
	\caption{Evaluation de la catégorisation par k-means et k-medoids de $\mathcal{D}_{arcpa}$ avec le nombre de clusters prédéfini à $K=3$.} \label{tab:similarite:validation-supervisee-k3}
\end{table}

Les valeurs de silhouette se reflètent plus sur les indices ARI et NMI, mais moins sur la F1-mesure. $Dis_M$ et $Dis_{cos}$ semblent les mieux adaptées pour le k-means, et $Dis_{cos}$ et $Dis_{jaccard}$ pour les k-medoids. Suivant les métriques ARI et NMI, 


\subsubsection{Nombre de clusters déterminé automatiquement}
Nous comparons ici l'emploie de la statistique d'écart et de la silhouette pour la détermination du nombre de cluster pour chaque distance. La sélection est effectué pour les valeurs 2 et 30. Même si $Dis_m$ retrouve le nombre attendu avec le k-means, $4$ est la plus valeur la plus choisie par les distances, et elle semble meilleure avec une F1-mesure maximale à 0.551 et un ARI de 0.398 avec $Dis_{cosine}$. Les valeurs déterminées (entre 2 et 6) sont très proches de la valeur attendue 3 pour toutes les distance.  Notons aussi que l'efficacité de $Dis_M$ et $Dis_{cos}$ reste presque aussi élevée qu'avec un $K$ prédéfini (Tableau \ref{tab:similarite:validation-supervisee-k3}). $Dis_M$ et $Dis_{cos}$ semblent ainsi former de bonnes combinaison avec respectivement les k-means et les k-medoids. Par ailleurs, seules $Dis_{manhattan}$ et $Dis_{wmd}$ obtiennent de très faibles valeurs des indices supervisés ARI et NMI. Même si ces valeurs sont inférieures pour les autres distances, ces dernières, en particulier $Dis_M$, $Dis_{jaccard}$ et $Dis_\textbf{cos}$, parviennent quand même à un bon compromis entre les 3 critères ARI, NMI et F1. $Dis_{jaccard}$ et $Dis_{cos}$ sont efficaces autant avec les k-means qu'avec les k-medoids, et elles parviennent à associer une bonne validation non-supervisée (silhouette) à une bonne validation non-supervisée.

\begin{table}[!htb]
	\centering \scriptsize
	\begin{tabular}[pos]{|l|l|c|c|c|c|c|c|c|}
		\hline
		\textbf{Distance}& \textbf{Algorithme}& \textbf{K}& \textbf{Silhouette}& \textbf{ARI} & \textbf{NMI} & \textbf{R} & \textbf{P} & \textbf{F1} \\ \hline
		dis\_m          & kmeans    & 3 & 0.438      & 0.407 & 0.423 & 0.552  & 0.654     & 0.599 \\ \hline
		dis\_m          & kmedoids  & 6 & 0.453      & 0.359 & 0.395 & 0.298  & 0.669     & 0.413 \\ \hline
		dis\_braycurtis & kmeans    & 4 & 0.473      & 0.383 & 0.407 & 0.446  & 0.658     & 0.532 \\ \hline
		dis\_braycurtis & kmedoids  & 5 & 0.448      & 0.344 & 0.375 & 0.331  & 0.645     & 0.437 \\ \hline
		dis\_cosine     & kmeans    & 4 & 0.528      & 0.383 & 0.407 & 0.446  & 0.658     & 0.532 \\ \hline
		dis\_cosine     & kmedoids  & 4 & 0.526      & 0.398 & 0.421 & 0.464  & 0.680     & 0.551 \\ \hline
		dis\_euclidean  & kmeans    & 5 & 0.478      & 0.365 & 0.395 & 0.341  & 0.670     & 0.452 \\ \hline
		dis\_euclidean  & kmedoids  & 5 & 0.456      & 0.313 & 0.346 & 0.335  & 0.619     & 0.434 \\ \hline
		dis\_jaccard    & kmeans    & 4 & 0.570      & 0.367 & 0.391 & 0.439  & 0.643     & 0.522 \\ \hline
		dis\_jaccard    & kmedoids  & 4 & 0.560      & 0.389 & 0.412 & 0.451  & 0.666     & 0.538 \\ \hline
		dis\_manhattan  & kmeans    & 4 & 0.482      & 0.376 & 0.400 & 0.452  & 0.657     & 0.535 \\ \hline
		dis\_manhattan  & kmedoids  & 5 & 0.452      & 0.368 & 0.397 & 0.345  & 0.675     & 0.456 \\ \hline
		dis\_pearson    & kmeans    & 2 & 0.611      & 0.054 & 0.072 & 0.746  & 0.453     & 0.564 \\ \hline
		dis\_pearson    & kmedoids  & 2 & 0.171      & 0.152 & 0.166 & 0.598  & 0.482     & 0.534 \\ \hline
		wmdistance      & kmedoids  & 2 & 0.332      & -0.016 & 0.002 & 0.545  & 0.397     & 0.459 \\ \hline
	\end{tabular}
	\caption{Evaluation de la catégorisation par k-means et k-medoids de $\mathcal{D}_{arcpa}$ avec détermination du nombre de clusters avec la silhouette.} \label{tab:similarite:validation-supervisee-optKbySilhouette}
\end{table}

La Figure \ref{fig:similarite:optimalK_with_silhouette_kmeans_dis_m_K2-30} montre l'évolution de la valeur de la silhouette en fonction du nombre de clusters pour $Dis_M$ utilisé dans le k-means. 

\begin{figure}[!htb]
	\centering \includegraphics[width=0.6\textwidth]{optimalK_with_silhouette_kmeans_dis_m_K2-30.png} \hfil
	%\includegraphics[width=0.49\textwidth]{eval-mlp.png}
	
	\caption{Evolution de la silhouette pour le k-means et la distance apprise}\label{fig:similarite:optimalK_with_silhouette_kmeans_dis_m_K2-30}
\end{figure}
 

\subsubsection{Autres algorithmes de catégorisation}
Avec la représentation sélectionnée TP-NGL, nous appliquons d'autres algorithmes de catégorisation sur $\mathcal{D}_{arcpa}$ (Tableau \ref{tab:similarite:validation-supervisee-optKbySilhouette-autres_algos}). Seules le regroupement hiérarchique et les C-moyennes floues probabilistes parviennent à trouver un nombre de clusters (4) assez proche de ce celui attendu (3). Les forêts aléatoires ont une très basse F1-mesure du fait du grand nombre de clusters déterminé. Par ailleurs, la tâche ne semble pas correspondre pour de la catégorisation par densité qui tend toujours à mettre tous les documents dans un même cluster.
\begin{table}[!htb]
	\centering \scriptsize
	\begin{tabular}[pos]{|l|c|c|c|c|c|c|c|}
		\hline
		\textbf{Algorithme}& \textbf{K}& \textbf{Silhouette}& \textbf{ARI} & \textbf{NMI} & \textbf{R} & \textbf{P} & \textbf{F1} \\ \hline
		Spectral Clustering & 19 & 0.352 & 0.193 & 0.317 & 0.069 & 0.632 & 0.124 \\ \hline 
		DBSCAN & 2 & -1.000 & 0.000 & 0.000 & 1.000 & 0.398 & 0.570 \\ \hline 
		%Kmeans & 4 & 0.524 & 0.383 & 0.407 & 0.446 & 0.658 & 0.532 \\ \hline 
		Agglomerative Clustering & 4 & 0.475 & 0.355 & 0.381 & 0.428 & 0.567 & 0.487 \\ \hline 
		Probabilistic Fuzzy-CMeans & 4 & 0.521 & 0.394 & 0.417 & 0.444 & 0.657 & 0.530 \\ \hline 
		Random Forest & 11 & 0.272 & 0.228 & 0.303 & 0.127 & 0.598 & 0.210 \\ \hline 
	\end{tabular}
	\caption{Evaluation de la catégorisation par d'autres algorithmes de $\mathcal{D}_{arcpa}$ avec détermination du nombre de clusters avec la silhouette.} \label{tab:similarite:validation-supervisee-optKbySilhouette-autres_algos}
\end{table}

La silhouette des C-moyennes floues probabilistes a une décroissance plus rapide et plus monotone que notre implémentation des k-means (Tableaux \ref{fig:similarite:optimalK_with_silhouette_kmeans_dis_m_K2-30} et )

\begin{figure}[!htb]
	\centering \includegraphics[width=0.6\textwidth]{optimalK_with_silhouette_ProbabilisticFuzzyCMeans_K2-30.png} \hfil
	%\includegraphics[width=0.49\textwidth]{eval-mlp.png}
	
	\caption{Évolution de la silhouette pour les C-moyennes floues probabilistes}\label{fig:similarite:optimalK_with_silhouette_ProbabilisticFuzzyCMeans_K2-30}
\end{figure}

\subsection{Catégorisation des corpus non annotés manuellement}
Les matrices document-terme des autres catégories sont construites à base des mêmes représentations sélectionnées sur la catégorisation manuel (cf. \ref{sec:similarite:select-repr-optimal}). Le tableau \ref{tab:similarite:validation-nonsupervisee} présente les valeurs de silhouette obtenues par le k-means et le k-medoid sur ces matrices. Les nombres de clusters reste faible
\begin{table}[!htb]
	\footnotesize
	\begin{center}
	\begin{tabular}[pos]{|l|l|c|c|}
		\hline
		\textbf{Corpus}& \textbf{Distance}& \textbf{K} & \textbf{S}  \\ \hline
	\multirow{2}{*}{$\mathcal{D}_{acpa}$ (21$^*$)} & $Dis_{cos}$  & 3 & 0.525 \\ \cline{2-4}
	& $Dis_{M}$ & 2 & 0.770\\ \hline
	\multirow{2}{*}{$\mathcal{D}_{concdel}$ (30)}  & $Dis_{cos}$  & 4 & 0.697  \\ \cline{2-4}
	& $Dis_{M}$ & 4 & 0.652\\ \hline
	\multirow{2}{*}{$\mathcal{D}_{danais}$ (198)}  & $Dis_{cos}$  & 5 & 0.426 \\ \cline{2-4}
	& $Dis_{M}$ & 4 & 0.403\\ \hline
	\multirow{2}{*}{$\mathcal{D}_{dcppc}$ (91)}  & $Dis_{cos}$  &  2 & 0.549 \\ \cline{2-4}
	& $Dis_{M}$ &2 & 0.451\\ \hline
	\multirow{2}{*}{$\mathcal{D}_{doris}$ (59)}  & $Dis_{cos}$  & 5 & 0.549 \\ \cline{2-4}
	& $Dis_{M}$ & 2 & 0.509\\ \hline
	\multirow{2}{*}{$\mathcal{D}_{styx}$ (50)}  & $Dis_{cos}$  & 4 & 0.572 \\ \cline{2-4}
	& $Dis_{M}$ &2 & 0.694\\ \hline
	\end{tabular}
\end{center}

	$^*$ Nombre de documents

	\caption{Evaluation non-supervisée des k-medoids sur $\mathcal{D}_{acpa}, \mathcal{D}_{concdel}, \mathcal{D}_{danais}, \mathcal{D}_{dcppc}, \mathcal{D}_{doris}, \mathcal{D}_{styx}$.} \label{tab:similarite:validation-nonsupervisee}
\end{table}

%\subsection{Terminologie des clusters générés}

\section{Conclusion}
\label{sec:similarite:conclusion}
Les circonstances où est faite une catégorie de demande doivent être découvertes afin de rapprocher les litiges non décidés des cas similaires de la jurisprudence. Les circonstances factuelles catégorisent les décisions mais sont illimités car ils correspondent aux faits courants de la vie. Ce problème ne pouvant pas être traité par classification supervisée (difficulté d'annotation), ce chapitre l'aborde comme une tâche de catégorisation non supervisée de document en explorant l'usage des algorithmes k-means et k-medoids, ainsi que diverses représentations de texte, et métriques de similarité. L'objectif étant de construire des groupes de documents qui partagent des circonstances factuelles similaires, à partir d'un corpus de décisions comprenant toutes une catégorie donnée de demande. La proposition faite ici est double: (i) l'apprentissage d'une distance sur le corpus donné en considérant qu'un document peut être obtenu par la transformation de tout autre document, (ii) l'exploitation de la faible quantité de catégorisation manuelle  pour sélectionner, par la silhouette moyenne, la représentation de texte qui correspond au mieux à la sémantique des circonstances factuelles. Le schéma sélectionné permet de transformer de nouveaux corpus non annotés afin d'y découvrir les circonstances factuelles par catégorisation non supervisée. Les expérimentations montre une amélioration considérable par rapport au modèle de base TF-IDF. La silhouette reste néanmoins faible. Ce qui signifie que la factorisation de matrice non négative est efficace mais il faudrait la combiner avec de meilleurs modèles vectoriels. Néanmoins cette sélection de représentation permet d'obtenir une assez bonne efficacité de catégorisation sur le corpus annoté (0.626 et 0.551 de F1-mesure, et 0.44 et 0.39 de ARI avec le kmedoids pour un nombre de clusters respectivement prédéfini et automatiquement déterminé). Ces résultats se traduisent aussi sur la largeur moyenne de la silhouette autant pour le le corpus annoté (0.448 et 0.456 respectivement) que pour les six corpus non annotés utilisés (entre 0.403 et 0.770 pour toutes les catégorie). Par ailleurs, la métrique apprise s'accorde mieux avec les k-means que les autres distances selon les différents indices de validation, et même pour la détermination du nombre de clusters.

Les algorithmes k-means et k-medoids ne sont pas toujours les plus efficaces pour la catégorisation de documents. L'expérimentation de meilleurs algorithmes devrait améliorer les résultats obtenus, comme par exemple l'intégration de la modélisation thématique FNM dans la catégorisation comme par exemple \citet{xie2013MGCTM} le fait avec l'allocation latente de Dirichlet. Par ailleurs, la représentation vectorielle n'est sélectionnée que sur un seule corpus pour les expérimentations de ce chapitre. Il serait intéressant d'annoter plusieurs corpus, pour améliorer la sélection en considérant la représentation vectorielle qui correspond en moyenne aux circonstances factuelles sur l'ensemble de ces catégorisations manuelles.