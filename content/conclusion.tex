\chapter*{Conclusion générale}
\chaptermark{Conclusion générale}
\phantomsection
\addcontentsline{toc}{chapter}{Conclusion générale}
\label{chap:conclusion}

%\textcolor{red}{Pourquoi n'avoir pas utilisé des méthodes de deep learning la thèse? disponibilité des approches de l'état de l'art, peu de données labellisées.}

\section{Synthèse des contributions}
\label{sec:conclusion:contributions}
Cette thèse  porte essentiellement sur la proposition et l'exploration d'approches adressant des problèmes d'analyses de données textuelles rencontrés lors de l'étude de corpus jurisprudentiels par des experts juristes. Trois problèmes principaux y sont abordés. Premièrement, l'annotation, dans les documents, des sections de textes et des entités juridiques, est traitée afin d'aider les experts à se repérer dans le document et à améliorer leur recherche de décisions judiciaires. Le chapitre \ref{chap:structuration}  démontre empiriquement, sur des documents annotés manuellement, l'efficacité de l'application de modèles probabilistes d'étiquetage de séquences, HMM et CRF, sur les deux tâches. Par la suite, l'extraction de données relatives aux demandes, suivant leur catégorie juridique, est discutée dans les chapitres \ref{chap:quanta} et \ref{chap:sensresultat}. Le problème impose d'effectuer les extractions pour une catégorie de demandes à la fois car il est impossible d'annoter suffisamment de données pour toutes les catégories prédéfinies. Pour cela, nous proposons de filtrer à l'entrée les documents de la catégorie à traiter par une classification binaire. Ensuite, nous proposons une approche  identifiant les attributs des demandes à l'aide de termes-clés prédéfinis et appris. Cette méthode, bien que dépendante d'heuristiques, parvient à reconnaître un grand nombre de demandes avec plus ou moins de difficultés selon les catégories traitées. Ensuite, la classification de documents est expérimentée comme approche plus généraliste. Sur l'ensemble des algorithmes explorés, les extensions de l'analyse PLS, appliquées ici pour la première fois sur du texte, démontrent une efficacité proche de celle du meilleur algorithme testé, l'arbre de décision. L'utilité de la restriction des documents à des passages relatifs à la catégorie est aussi observée empiriquement. Enfin, le chapitre \ref{chap:similarite} aborde la problématique de similarité entre deux textes dans un contexte de catégorisation non supervisée des documents. Le but est ici de révéler les circonstances factuelles faisant appel à une catégorie de demande particulière. Une approche d'apprentissage de distance est proposée : elle repose sur le coût d'une transformation d'un des deux textes en l'autre. Cette distance est comparée à d'autres métriques avec l'algorithme des K-moyennes dans des expérimentations qui explorent différents aspects des problèmes de regroupement comme la détermination du nombre de clusters ou la représentation de documents. En somme, les problèmes abordés sont variés et très importants dans le métier des experts juristes. Le chapitre \ref{chap:demo} illustre en l'occurrence l'application des propositions de cette thèse à l'analyse descriptive d'un corpus de décision. 


\section{Critique du travail}
\label{sec:conclusion:critique}
%Au delà des nombreuses problématiques abordées et expérimentations discutées, cette thèse reste limitée par son niveau de contribution théorique d'une part. La proposition globale est une chaîne de traitement employant à chaque niveau des approches soit existantes soit plus techniques. Aussi, un très grand nombre de méthodes de la littérature sont absentes, surtout les plus récentes; ceci est dû fait à l'ampleur du travail et à la multitudes d'approches existantes.  D'autre part, les études menées ont rencontrées comme obstacles la disponibilité d'exemples de référence annotées manuellement. La lenteur et la pénibilité de l'identification des informations à la main se traduit par la faible quantité des données employées pour les expérimentations. De plus, ne disposant que d'un expert, le degré d'accord entre annotateurs n'a été analysé que pour la première problématique de reconnaissance d'entités et de sections. Par conséquent, certaines subtilités propres à l'expert ou des données manquées lors de l'annotation manuelle, peuvent biaiser les résultats observés. Néanmoins, les nombreux résultats obtenus servent de base pour la continuité des études. 

Cette thèse est limitée par la faible quantité des données employées pour les expérimentations. Cette dernière est révélatrice de la lenteur et de la pénibilité liée à l'annotation manuelle des jeux de données d'évaluation. Par conséquent, il est difficile d'avoir une estimation du taux de données manquées par l'expert ou du degré de différence entre son annotation manuelle et celles qu'auraient pu réaliser d'autres juristes. De plus, ne disposant la plupart du temps que d'un expert, l'annotation manuelle n'a été évaluée que pour le problème de détection des sections et entités juridiques. Par ailleurs, un grand nombre de méthodes de la littérature n'ont pas été expérimentées pour deux raisons principales. D'une part, la littérature regorge de très nombreuses méthodes répondant aux divers problèmes traités ici. D'autre part, certaines méthodes intéressantes ne sont pas adaptées à nos conditions expérimentales. Par exemple, les réseaux neurones profonds sont réputés gourmands en données annotées mais nous n'en disposons que de très peu. Nous avons aussi expliqué par exemple que les méthodes proposées pour l'extraction des évènements exploite une annotation manuelle qui renseigne sur la position exacte où se trouve les données ciblées dans le texte. Les données d'apprentissage pour l'identification des demandes sont répertoriées, au contraire, dans un tableau à l'extérieur des documents d'origine.


\section{Travaux futurs de recherche}
\label{sec:conclusion:extensions}
Les propositions partagées dans la conclusion des chapitres \ref{chap:structuration} à \ref{chap:demo} pour poursuivre les travaux peuvent être résumées en 4 catégories principales. En premier, l'exploration de méthodes plus récentes que celles étudiées dans ce manuscrit permettra d'étendre les résultats expérimentaux. Ensuite, la formalisation des problèmes abordés permettra de définir des approches plus théoriques. Par exemple, la formalisation des demandes comme des relations, entre quantum demandé et quantum accordé, permettra d'explorer le cadre probabiliste et neuronal de la littérature en matière d'extraction de relations. Puis, l'exploration d'autres formulations des problèmes permettra probablement de découvrir des méthodes plus efficaces. Par exemple, on peut percevoir la détermination des circonstances factuelles comme une tâche de modélisation de thématiques (\textit{topic modeling}). Enfin, les études menées méritent d'être étendues sur d'autres aspects. Par exemple, la détection d'entités juridiques doit être étendue à la tâche de résolution qui unifie automatiquement les différentes occurrences d'une entité sous un identifiant unique. Cette résolution est importante pour l'automatisation d'autres tâches du métier comme l'anonymisation des décisions judiciaires.

Il faut aussi remarquer qu'il reste encore des types d'information dont le problème d'extraction n'est pas abordé par cette thèse. Par exemple, les raisons qui  font pencher les juges en faveur d'une décision sur une demande, sont indispensables pour être capable d'anticiper la prise de décision des juges. L'extraction des raisons concernera l'identification et l'analyse des arguments des parties et les motivations des juges.

 Par ailleurs, il faudra aussi mieux évaluer la qualité des annotations manuelles expertes ce qui révélera le niveau d'accord non seulement sur les données annotées mais aussi sur leur perception des informations ciblées comme les circonstances factuelles qui semblent être subjectives. 

Cette thèse est l'un des premiers travaux de recherche d'une telle diversité de problèmes sur les décisions de justice françaises. Ainsi, elle ouvre la voie à bien des problématiques comme l'analyse des réseaux de normes, l'anonymisation des décisions, ou l'analyse des arguments, déjà largement étudiés dans d'autres pays, notamment aux États-Unis. En cela, cette thèse encourage la recherche en analyse de données textuelles à s'intéresser à l'analyse automatique de la jurisprudence française dont les défis, la disponibilité d'un grand volume de données et la richesse du domaine judiciaire rendent les applications attractives. Les cas d'utilisation des données extraites sont très nombreux pour la recherche en droit, pour l'aide à la décision des juristes, pour l'enseignement du droit, mais aussi et surtout pour l'accessibilité des profanes au droit par une estimation automatique de leurs risques judiciaires.
