%\chapter{Analyse sémantique de Corpus Textuel par Traitement Automatique du Langage Naturel}
\chapter{Analyse automatique de corpus judiciaires}
\label{chap:literature}


% justice prédictive: limites: fiabilité mathématiques, exhaustivité, résultats différents d'un outils à un autre, quelles données analysées? % necessité: réduire le risque d'erreur d'une 
L'étude bibliographique de ce chapitre est focalisée sur l'application de techniques d'analyse de données textuelles judiciaires. Une synthèse bibliographique plus technique sur les algorithmes de fouille de texte est détaillée dans les chapitres qui traitent, dans la suite, des méthodes que nous avons mises en \oe{}uvre. Plus précisément, suivant la structure du présent chapitre,  il s'agit des chapitres \ref{chap:structuration} et \ref{chap:quanta} pour l'extraction d'information, du chapitre \ref{chap:sensresultat} pour la classification des documents, et du chapitre \ref{chap:similarite} pour la similarité entre documents.

\section{Introduction}

% rapport aux théories juridiques: réalisme vs formalisme
Les deux grands paradigmes de jugement se distinguent par l'importance qu'ils accordent aux règles juridiques \citep{tumonis2012legalrealism}. D'une part, les adeptes du Formalisme Juridique, plus pertinent dans le droit civil, considèrent que toutes les considérations normatives ont été incorporées dans les lois par leurs auteurs. D'autre part, l'école du Réalisme Juridique, plus proche du \og \textit{Common Law}\fg{}, permet un pouvoir discrétionnaire entre les jugements en raisonnant selon le cas. Les premières tentatives d'anticipation des comportements judiciaires s'appuyaient sur une formalisation des lois. Il en est né le \og droit computationnel \fg{}, qui est une sous-discipline de  l'\og informatique juridique\footnote{Application des techniques modernes de l'informatique à l'environnement juridique, et par conséquent aux organisations liées au droit.} \fg{}. Il  s'intéresse, en effet, au raisonnement juridique automatique axé sur la représentation sémantique riche et plus formelle de la loi, des régulations, et modalités de contrat \citep{love2005computationallaw}. Il vise à réduire la taille et la complexité de la loi pour la rendre plus accessible. Plus précisément, le \og droit computationnel \fg{} propose des systèmes répondant à différentes questions, comme \og Quel montant de taxe dois-je payer cette année? \fg{} (planification juridique), \og Cette régulation contient-elle des règles en contradiction\fg{} (analyse réglementaire),  \og L'entreprise respecte-t-elle la loi?" (vérification de la conformité) \citep{Genesereth2015computationallaw}. Les techniques pro Formalisme Juridique étaient déjà critiquées au début des années 60, parce qu'excessivement focalisées sur les règles juridiques qui ne représentent qu'une partie de l'institution juridique \citep{llewellyn1962jurisprudence}. Pour analyser le comportement judiciaire, plusieurs variables plus ou moins contrôlables, comme le temps, le lieu et les circonstances, doivent aussi être prises en compte \citep{ulmer1963quantitative}. Etant donné que les juristes s'appuient sur la recherche de précédents, \citet{ulmer1963quantitative} conseille de se concentrer sur les motifs réguliers que comprennent les données pour réaliser des analyses quantitatives. Il est possible d'exploiter la masse de décisions pour identifier de telles régularités car une collection suffisante d'une certaine forme de données révèle des motifs qui une fois observés sont projetables dans le futur \citep{ulmer1963quantitative}. Il s'agit de raisonnements à base de cas qui se distinguent de ceux à base de règles.

% Généralités sur l'application du text mining / IA en général aux documents juridique: objectifs, données, conférences, commercialisation, activités gouvernementales, inquiétudes ...
Les premiers outils automatiques d'anticipation des décisions étaient généralement des systèmes experts juridiques. Ces derniers résonnent  sur de nouvelles affaires en imitant la prise de décision humaine par la logique en général et souvent par analogie. Ils s'appuient sur un raisonnement à base de règles, c'est-à-dire à partir d'une représentation formelle des connaissances des experts ou du domaine. En droit, il s'agit de la connaissance qu'a l'expert des normes juridiques, et de l'ordre des questions à traiter lors du raisonnement sur un cas (appris par expérience). Le modèle explicite de domaine nécessaire ici se trouve dans une base de connaissances où les normes juridiques sont représentées sous forme de \og SI ... ALORS ...\fg{}, et les faits sont généralement représentés dans la logique des prédicats. Un système expert juridique doit s’appuyer sur une base de connaissances juridiques exhaustive et disposer d’un moteur d’inférence capable de trouver les règles pertinentes et le moyen efficace, par déduction, de les appliquer afin d’obtenir la solution du cas d'étude aussi rapidement que possible. Les systèmes experts ont échoué dans leur tentative de prédire les décisions de justice \citep{leith2010risefall}. La première raison découle de ce que \citet{Berka2011rbr-cbr} a appelé le \og goulot d'acquisition de connaissances \fg{} c'est-à-dire le problème d'obtention des connaissances spécifiques à un domaine d’expertise sous la forme de règles suffisamment générales. L'autre raison tient à l'interprétation ouverte du droit et à la complexité de la formalisation applicable sans tenir compte des particularités de l'affaire.

Contrairement au raisonnement à base de règles, le raisonnement à base de cas concerne une recherche de solution, une classification ou toute autre inférence pour un cas courant à partir de l'analyse d'anciens cas et de leurs solutions \citep{moens2002case-basedreasoning}. Un tel système juridique résout les nouveaux cas en rapprochant les cas déjà réglés et en adaptant leurs décisions \citep{Berka2011rbr-cbr}. Le raisonnement fondé sur des cas connaît un succès croissant dans la prédiction de l'issue d'affaires, davantage aux États-Unis qu'ailleurs. Pour exemple, \citet{katz2014predicting} entraînent des forêts aléatoires \citep{breiman2001randomforest} sur les cas de 1946-1953 pour prédire si la Cour Suprême des États-Unis infirmera ou confirmera une décision de juridiction inférieure. Leur approche parvient à prédire correctement 69,7\% des décisions finales pour 7700 cas des années 1953-2013. Ils ont légèrement amélioré ce résultat plus récemment en augmentant le nombre d'arbres et la quantité de données \citep{katz2017predictsupremecourt}. Toujours pour la prédiction des décisions de la Cour Suprême des Etats-Unis, \citet{waltl2017predictgermantaxlaw} utilisent des techniques de traitement automatique du langage naturel (TALN) pour extraire moins d'attributs caractéristiques de décision que \citep{katz2014predicting} à partir des décisions d'appel de la Cour Fiscale Allemande (11 contre 244). Ils obtiennent des valeurs de $F_1$-mesures entre 0,53 et 0,58 (validation croisée à 10 itérations) pour la prédiction  de la confirmation ou l'infirmation d'un jugement en appel avec un classifieur bayésien naïf.  Par ailleurs, \cite{Ashley2009classifCases} ont obtenu une précision de 91,8\% pour la prédiction de la partie (plaignant/défendeur)  qui sera favorisée à l'issue d'affaires d'appropriation illicite de secrets commerciaux.

% introduction des sections suivantes
Notre objectif est d'alimenter les analyses quantitatives de corpus jurisprudentiels en proposant des méthodes d'extraction de connaissances pertinentes telles que les références des affaires (juge, date, juridiction, etc.), les règles juridiques associées, les demandes des parties, les réponses des tribunaux, et les liens entre ces données. Les juges apportent une réponse à chaque demande. Par conséquent une partie peut voir chacune de ses demandes individuellement acceptée ou rejetée partiellement ou entièrement. Un juriste sera donc plus intéressé à formuler et défendre les demandes qui ont de meilleures chances d'être acceptées pour un type de contentieux précis plutôt que de prévoir une victoire du procès. C'est la raison pour laquelle notre analyse se situe à un niveau de granularité plus fin (la demande), contrairement aux travaux sur la prédiction qui traitent d'un résultat global sur la décision (par ex. confirmer/infirmer ou gagner/perdre).  Un des postulats considérés dans cette thèse est que l'identification de ces diverses connaissances est possible par l'analyse sémantique des textes judiciaires grâce aux méthodes du TALN. Cependant, l'application de ces techniques exigent certaines adaptations pour surmonter les divers défis décrits par \citet{narazenko2017legalnlpintro}: textes très longs et en grande quantité, corpus régulièrement mis à jour, influence subjective de facteurs sociaux et d'opinions politiques, couverture de problématiques économiques, sociales, politiques très variées, langage complexe, etc.. Dans la suite de ce chapitre, nous passons en revue des travaux qui ont été menés dans ce sens pour traiter des problématiques proches des nôtres, en particulier celles décrites dans l'introduction (\ref{subsec:intro:ie}). 

\section{Annotation et extraction d'information}

L'annotation consiste à enrichir les documents pour les préparer à des analyses, faciliter la recherche d'affaires pertinentes, et faire la lumière sur des connaissances linguistiques sous-jacentes au raisonnement juridique. Les éléments annotés peuvent être de courts segments de texte mentionnant des entités juridiques \citep{Waltl2016lexia, wyner2010extractlegalelts} comme la date, le lieu (juridiction), les noms de juges, des citations de loi.  L'annotation de passages plus longs consiste à identifier des instances de concepts juridiques plus complexes comme les faits \citep{wyner2010extractlegalelts, wyner2010casefactors, Shulayeva2017recognfactprincip}, les définitions \citep{Waltl2016lexia,waltl2017legaliegerman}, des citations de principes juridiques \citep{Shulayeva2017recognfactprincip}, ou des arguments \citep{WynerMoens2010mineargument}. 

Différentes méthodes ont été expérimentées pour la reconnaissance d'information dans les documents judiciaires. La plupart reposent sur l'entraînement d'algorithmes d'apprentissage automatique supervisé sur un ensemble d'exemples annotés manuellement (résultats attendus). Parmi ces algorithmes, on retrouve par exemple les modèles probabilistes HMM (Modèles Cachés de Markov, cf. \ref{sec:structuration:litérature-HMM}) et CRF (Champs Aléatoires Conditionnels, cf. \ref{sec:structuration:litérature-CRF}) que nous étudions au chapitre \ref{chap:structuration}. Ces modèles peuvent être combinés à d'autres approches dans un système global. En effet, après avoir segmenté les documents à l'aide d'un modèle CRF, \citet{dozier2010legalnerr} ont par exemple combiné plusieurs approches pour reconnaître des entités dans les décisions de la Cour Suprême des États-Unis. Ils ont défini manuellement des détecteurs distincts à base de règles pour identifier séparément la juridiction (zone géographique), le type de document, et les noms des juges, en plus de l'introduction d'une recherche lexicale pour détecter la cour, ainsi qu'un classifieur entraîné pour reconnaître le titre. Ces différents détecteurs ont atteint des performances prometteuses, mais avec des rappels limités entre $ 72 \% $ et $ 87 \% $. Suivant la complexité des éléments à extraire, un système peut exploiter un lexique pour les motifs simples et non-systématiques (indicateurs de mentions de résultats ou de parties) et des règles pour des motifs plus complexes et systématiques (e.g., noms de juges, énoncés de décisions) \citep{Waltl2016lexia,waltl2017legaliegerman, wyner2010extractlegalelts}. \cite{cardellino2017legalNERCL} ont par ailleurs utilisé un modèle CRF et des réseaux de neurones pour la reconnaissance d'entités nommées juridiques dans des jugements de la Cour Européenne des Droits de l'Homme.
%\textbf{(comment seb: pourquoi faire ?)}
Ils définissent une hiérarchie des entités nommées distinguant au niveau 1, les entités nommées et des non-entités,  spécialisées par 6 classes au niveau 2 (par exemple, Personne, Document), spécialisées par 69 classes au niveau 3 (par exemple, Rôle Juridique, Règlement), spécialisées par 358 classes au niveau 4 (par exemple Juge, Code Juridique). Les basses performances qu'ils rapportent sur le corpus juridique illustrent bien la difficulté de la détection d'entités juridiques dans les décisions judiciaires ($F_1$-mesures de 0.25, 0.08, 0.03 en moyenne respectivement pour les niveaux 2, 3, 4). Plus récemment encore, \citet{andrew2018legalNerAndRelation} proposent une approche pour l'extraction d'entités nommées d'une transaction d'investissement\footnote{Entités: Personne, Nom, Adresse, Société Principale, Société Secondaire, Rôle, Fonction, Type Société.} et des relations qu'elles partagent dans des décisions du Luxembourg rédigées en français. Ils combinent un modèle CRF pour les entités à  une grammaire GATE JAPE \citep{thakker2009gatejape} pour les relations, et obtiennent un faible taux d'erreur pour le CRF de 3.12\%. % ?? la formule de calcul du taux d'erreur n'a pas été donnée. Peut^-être 100 - Accuracy?? %\textbf{(comment seb: ici aussi préciser la finalité du système et ajouter ses performances)}

 Pour la détection des arguments, par contre, \citet{moens2007NBvsMaxent4arguments} proposent une classification binaire des phrases: \textit{argumentative} / \textit{non argumentative}. Ils comparent notamment le classifieur bayésien multinomial et le classifieur d'entropie maximum tout en explorant plusieurs caractéristiques textuelles. \citet{mochales2008contextfreegrammararg} proposent, pour la même tâche, une méthode d'extraction basée sur une formalisation de la structure des arguments dans les jugements par une grammaire sans contexte. 

% argument (Grammaire) :\cite{WynerMoens2010mineargument} http://wyner.info/research/Papers/WynerMochalesPalauMoensMilward2009.pdf
% terminologie : https://pdfs.semanticscholar.org/4d49/2d103672723d5683e4fc5b468e49ffaece3b.pdf

\section{Classification des jugements}
La classification de texte permet d'organiser un corpus en rangeant les documents dans des catégories généralement prédéfinies par des experts. Pour la classification des décisions, le principe des propositions de la littérature est d'entrainer un modèle statistique traditionnel sur une représentation des documents généralement définie à partir des connaissances du domaine.% Par exemple, \citet{katz2014predicting} prédéfinissent des valeurs d'attributs pour décrire les décisions à partir de connaissances sur les tribunaux et les juges (opinions politiques, origine de l'affaire, identifiant du juge, raison et sens du dispositif de la cour inférieure, etc.) et  \cite{Ashley2009classifCases} identifient, par classification, des facteurs pouvant influencer la décision. 
 Par exemple, par classification binaire avec une Machine à Vecteurs de Support (SVM) \citep{vapnik1995statlearning} à noyau linéaire (cf. \ref{sec:sens-resultat:svm}),  \citet{Aletras2016predictDecisionECHR} identifient s'il y a eu une violation d'un article donné de la convention des droits de l'homme sur les jugements de la Cour Européenne des Droits de l'Hommes (CEDH)\footnote{HUDOC ECHR Database: \url{http://hudoc.echr.coe.int}.}. Les vecteurs représentant les documents sont construits sur la base des 2000 n-grammes les plus fréquents. Certaines composantes  sont les fréquences normalisées des n-grammes sélectionnés (modèle sac-de-mots \citep{salton1975BoW, salton1983modernIR_BoW}), calculées distinctement pour différentes parties du document (Procédure, Circonstances, Faits, Loi applicable, la Loi et le document entier); ce qui résulte en une matrice document-terme $C$. D'autres composantes sont définies par la fréquence des thématiques extraites par une catégorisation non supervisée (\textit{clustering}) avec la similarité cosinus des n-grammes les plus fréquents représentés par leurs vecteurs dans $C$, i.e. le vecteur de leurs scores d'occurrence dans les différentes parties précédemment citées du document. \citet{Aletras2016predictDecisionECHR} obtiennent une précision moyenne de 79\% sur les 3 articles qu'ils ont manipulés. 
%\textbf{(comment seb : je ne comprends pas "et le cluster de leur vecteur" à reprendre, ne pas hésiter à détailler un peu plus)}
Notons tout de même la sélection des régions particulières (circonstances, faits, lois, ...) du document à partir desquelles sont extraits les n-grammes. Cette sélection est un ajustement de la représentation des textes qui paraît nécessaire pour obtenir de bons résultats. La structuration préalable des documents est ainsi utile pour réduire le bruit qui occupe généralement plus d'espace que les passages ou éléments d'intérêt.  \citet{medvedeva2018echrCristalBall} étendent ces travaux à neuf articles de loi, tout en montrant empiriquement, entre autres, la possibilité de prédire la violation des articles sur des périodes futures à celles couvertes par les données utilisées lors des phases d'entraînement. \cite{sulea2017legalEnsSVM} traitent, d'autre part, l'identification des résultats dans des arrêts \footnote{Documents de \url{https://www.legifrance.gouv.fr}.} de la Cour Française de Cassation. Après un essai \citep{Sulea2017predictareadecision} avec un SVM entrainé sur une représentation des documents par le modèle  TF-IDF \citep{salton1988term-weighting}, ils améliorent les résultats à l'aide d'un classifieur ensembliste de SVM à probabilité moyenne,
%\textbf{(comment seb : à détailler)}
 parvenant à des $F_1$-mesures de plus de 95\% \cite{sulea2017legalEnsSVM}. Un classifieur SVM à probabilité moyenne combine plusieurs modèles SVM dits \og faibles \fg{} (ou de base)  entrainés chacun sur un sous-ensemble de la base d'apprentissage.
%soit avec moins de variables, soit avec moins d'instances que celes du corpus original. 
Lors de la prédiction, chacun des SVM estime une probabilité d'appartenance du document classifié à chaque classe. La classe votée pour le document est celle dont la  probabilité moyenne (robustement estimé par la médiane \citep{kittler1998combiningClassifiers}) est maximale.

Par ailleurs, \cite{Ashley2009classifCases} identifient les \og Facteurs \fg{} (\textit{factors} \citep{ashley1990modeling_factors}) qui s'appliquent à une affaire par classification de la description des faits de cette affaire. Les Facteurs sont en effet des questions juridiques qui se posent dans un domaine \citep{bench1997arguingwithwases_factors}. Par exemple, l'environnement d'enseignement CATO définit 26 Facteurs sur le domaine de la loi américaine sur le secret commercial (\textit{trade secret law}) notamment \textit{Unique-Product} (Le produit est-il unique?) et \textit{Non-Disclosure-Agreement} (Existait-il in accord de non-divulgation?). Ils sont utilisés dans des raisonnements à base de cas où ils sont définis sous forme de prédicats faisant abstraction des faits. Certains sont binaires (réponse "oui" ou "non").   Un Facteur s'applique à une affaire, si les faits de cette dernière contiennent le patron correspondant. Ils entraînent un classifieur (les plus-proches-voisins) pour un ensemble de 27/26 facteurs \footnote{Les facteurs catégorisent les faits\textcolor{red}{Qu'est ce qu'un facteur? Comment il s'applique à une décision? Est-il pertinent pour la prédiction de la décision? Exemple?}} prédéfinis pour identifier ceux qui s'appliquent à la décision. 
\textbf{(comment seb : détailler quelques exemples de facteurs, 'pour savoir s'il s'applique à la décision' tu veux dire pour savoir s'ils sont pertinents pour la prédiction de la décision ?)} 
La partie remportant le procès est par la suite prédite par un algorithme séquentiel qui compare les parties (plaignant et défendeur) suivant le niveau de préférence des questions juridiques dégagées par les facteurs observés dans la base d'entraînement. D'autres catégorisations sont tout aussi utiles pour faciliter la recherche d'information. Par exemple, \citet{Sulea2017predictareadecision,sulea2017legalEnsSVM} expérimentent la classification des décisions dans les formations judiciaires (par exemple, chambre civile, chambre commerciale, chambre sociale) ou l'identification de la période\footnote{Intervalle d'années dans laquelle la décision a été prononcée.} de la décision. 
 %{\textbf{(comment seb : prononcé ?)}}  
 La classification peut aussi servir à évaluer d'autres problématiques comme la similarité \citep{ma2018wmdchinesecase}.

\section{Similarité entre décisions judiciaires}
% https://scholar.google.com/scholar?oe=utf-8&client=firefox-b-ab&um=1&ie=UTF-8&lr&cites=2644458803665738328
% https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=6&cad=rja&uact=8&ved=2ahUKEwik05PbjdvdAhUI_qQKHU9UC6QQFjAFegQIAxAC&url=http%3A%2F%2Fweb2py.iiit.ac.in%2Fpublications%2Fdefault%2Fdownload%2Finproceedings.pdf.8d3930f256a00e9c.436f6d707574655f323031315f53757368616e74615f4b756d61722e706466.pdf&usg=AOvVaw3CQX2nPEbeTXt6LhlRoOj6
% quel sémantique fonde la similarité dans chaque travaux? ou comment est défini la similarite entre les documents (dans la sémantique experte) ?
% quelle métrique formalise / numérise / mesure la similarité?
% comment sont évalués les méthodes explorées? contexte d'utilisation et métriques d'évaluation?
% comment sont représentés les documents?

La similarité entre textes est indispensable pour des applications qui nécessitent de regrouper des textes traitant de sujets similaires, et séparer ceux dont les sujets sont différents. La  mesure de similarité doit être définie de sorte à rapprocher ou éloigner les documents suivant l'aspect sémantique que l'on souhaite révéler. \citet{nair2018judgsimassorule} exploitent les citations de lois et précédents\footnote{Les jugements du \og Common Law \fg{} citent des décisions antérieures similaires.} pour retrouver les textes juridiques qui ont une similarité. Ils analysent le réseau de 597 citations\footnote{Disponibles sur \url{https://indiankanoon.org}.} sous l'Acte 2000 des Technologies de l'Information (\textit{Information Technology Act, 2000\footnote{\url{https://www.meity.gov.in/content/information-technology-act-2000}.}}) dans des jugements indiens. Leur proposition est d'utiliser des règles d'association générées par l'algorithme Apriori \citep{agrawal1994algoApriori} pour regrouper les jugements susceptibles d'être cités ensemble. Cet algorithme recherche les ensembles singletons de citations suffisamment fréquentes (seuil nécessaire), puis fusionne de manière itérative les ensembles tant que la co-occurrence des citations de la fusion est suffisamment fréquente dans le réseau. Une règle d'association $\lbrace c_1,\dots,c_n \rbrace \rightarrow \lbrace c' \rbrace$ indique qu'une citation $c'$ est observable si l'on observe une co-occurrence d'un ensemble donné de citations $\lbrace c_1,\dots,c_n \rbrace$. A chaque règle est associé un score de confiance calculé à partir d'une métrique appelé score de support $sc()$ qui indique, pour un ensemble $\lbrace c_1,\dots,c_n \rbrace$, la fréquence de co-occurrence des citations de cet ensemble. Le support d'un singleton est sa fréquence d'occurrence. La similarité (\textcolor{red}{laquelle?}) est confirmée si le score de confiance de la règle  $conf(\lbrace c_1,\dots,c_n \rbrace \rightarrow \lbrace c' \rbrace) = sc(\lbrace c_1,\dots,c_n, \text{\textcolor{red}{c'}} \rbrace) / sc(\lbrace c' \rbrace)$ est suffisamment élevé.
%(\textbf{comment seb : donner des informations et une citation sur cet algo}). 
\citet{nair2018judgsimassorule} démontrent au travers de scénarios (aucune évaluation quantifiée de l'efficacité de l'approche n'est proposée) que les documents qui sont fréquemment cités ensemble sont similaires car traitant de thématiques proches. Cette relation permet par transitivité de retrouver les documents pertinents dans une base de données. 

Les métriques traditionnelles de similarité ne sont pas toujours très efficaces sur les décisions judiciaires. La raison peut être une représentation inadéquate des textes qui ne permet pas de traduire une fois comprise la notion de similarité telle qu'entendue dans la plupart des travaux. \citet{thenmozhi2017legalprecedretriev} comparent par exemple, l'utilisation de la similarité cosinus sur trois représentations différentes des jugements dans le cadre de la campagne de recherche d'affaires antérieures pertinentes  IRLeD@FIRE2017\citep{mandal2017IRLeD@FIRE2017}: (1) TF-IDF des concepts (noms), (2) TF-IDF des concepts et relations (verbes), (3) et la moyenne des plongements lexicaux \textit{Word2Vec} \citep{lemikolov2014word2vec} des concepts et relations. Au vu des résultats (0.1795, 0.178, 0.0755 de précision@10\footnote{\label{footnote:literature:PR_at_N} Précision@$N$, rappel@$N$: précision et rappel calculées sur les $N$ premiers résultats retournés par un système de recherche d'information.} et 0.681, 0.661, 0.435 de rappel@10\textsuperscript{\ref{footnote:literature:PR_at_N}} respectivement pour les méthodes 1, 2, et 3), la première représentation semble mieux capter la similarité contrairement à l'utilisation des verbes et de la représentation distribuée.  %(\textbf{comment seb : appuyer le propos à l'aide de la littérature, ref sur des performances, et citation de chiffre})
 \citet{ma2018wmdchinesecase} utilisent une forme de connaissances a priori définie dans des modèles de type ontologie pour estimer la similarité entre décisions. Ils proposent notamment d'aligner le document sur une ontologie des concepts et relations d'un corpus judiciaire. L'idée est de calculer la similarité sur un résumé du texte qui regroupe des aspects pertinents. Cette méthode permet ainsi de mieux capter la sémantique des jugements, d'avoir une meilleure précision, et de réduire la complexité temporelle inhérente à l'exploitation de longs documents notamment lors de l'utilisation de la \og distance du déménageur de mots \fg{} ou WMD (\textit{Word Mover's Distance}) de \citet{kusner2015wordmoverdist} (cf. \ref{sec:similarite:distances}). L'amélioration est observée sur une tâche de classification des jugements Chinois relatifs aux crimes de la circulation routière dans quatre catégories correspondant à des sentences d'emprisonnement\footnote{Détention, emprisonnement à durée déterminée de moins de 3 ans, emprisonnement de durée déterminée de 3 à 7 ans et emprisonnement de plus de 7 ans.} (précision de 90.3\% et 92.3\% pour le résumé contre 84.8\% et 82.4\% respectivement pour le document original). 

Toujours dans l'objectif d'une représentation pertinente des textes, \citet{kumar2011judgmentsimilarity} proposent quatre méthodes propres aux décisions judiciaires pour l'estimation de la similarité entre deux jugements  $x$ et $y$ de la Cour Suprême indienne:
\begin{enumerate}
\item \textit{all-term cosine similarity}: le cosinus de similarité entre les représentations TF-IDF de $x$ et $y$ dont tous les termes présents dans les jugements sont les dimensions.
\item \textit{legal-term cosine similarity}: le cosinus de similarité sur les réductions des dimensions précédentes uniquement aux termes apparaissant dans un dictionnaire juridique.
\item  \textit{bibliographic coupling similarity}: la similarité de couplages bibliographiques égal au nombre de citations de jugements communes à $x$ et $y$.
\item \textit{co-citation similarity}: la similarité de co-citation qui est le nombre de citations de $x$ et $y$ dans un même jugement. 
\end{enumerate}

 %La similarité étudiée par \citet{kumar2011judgmentsimilarity} est basée sur trois critères: la similarité sur la question discutée, la similarité sur les faits sous-jacents, et l'utilité du document pour les avocats cherchant des documents similaires à une décision donnée. 
 Bien qu'ils aient interprété les résultats sur de très faibles proportions des données utilisées (5/2430 et 18/2430), il en ressort que le cosinus de similarité avec les termes juridiques et le couplage bibliographique correspondent aux valeurs de similarité des experts, contrairement à la similarité basée sur tous les termes du corpus ou sur la co-citation. 
 \textbf{(comment seb : fournir les performances)}

En synthèse, la similarité entre documents est utilisée pour répondre à plusieurs tâches, comme par exemple, la recherche de décisions similaires \citep{thenmozhi2017legalprecedretriev}, le regroupement non-supervisé de jugements \citep{raghuveer2012legalclusteringLDA} et la classification supervisée de ces derniers \citep{ma2018wmdchinesecase}. Ces diverses applications définissent aussi la sémantique juridique liée à la notion de similarité. Parmi les questions liées à la conception d'une mesure de la similarité entre documents, on distingue: la sémantique experte qui fonde cette similarité, sa métrique de mesure, la représentation des documents, le contexte d'exploitation et les critères d'évaluation. %Diverses études menées sur la similarité démontrent l'importance de l'abstraction des textes par les concepts soit via l'alignement du document avec une ontologie, soit via la sélection de termes clés.

\section{Conclusion}
\label{sec:literature:conclusion}
%\subsection{Types d'approches appliquées}
En résumé, les travaux portant sur l'analyse automatique des décisions ont donné des résultats encourageants grâce aux éléments spécifiques aux affaires. Ces éléments peuvent être extraits des décisions grâce aux techniques de TALN et de fouille de texte.  L'analyse des données textuelles juridiques a pour but la structuration des documents, l'extraction d'information, et l'organisation sémantique de corpus. Le domaine est très actif depuis déjà plusieurs décennies, au point que des librairies de développement, spécifiques au domaine, commencent à voir le jour \citep{bommarito2018lexnlp}. Dans la littérature, nous remarquons que le concepteur investit un minimum d'ingénierie d'adaptation que ce soit pour la définition des caractéristiques pertinentes pour les modèles à apprentissage automatique, ou pour définir les règles pour les méthodes à base de règles ou à base de grammaire. Notons aussi l'effort d'évaluation quantitative avec la participation d'experts pour l'annotation d'exemples de référence même pour des tâches qui peuvent paraître subjectives comme la mesure de similarité.

%\subsection{Évaluation et qualité}

%\cite{Galgani2015lexa} montrent qu'il est possible en un temps raisonnable d'annoter des texte

